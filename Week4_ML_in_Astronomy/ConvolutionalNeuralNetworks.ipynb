{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9WtSa8lWkWd"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "### Author: [John F. Wu](https://jwuphysics.github.io/)\n",
        "\n",
        "Presented as a tutorial during [Session 19](https://github.com/LSSTC-DSFP/Session-19) of the [LSSTC Data Science Fellowship Program](https://www.lsstcorporation.org/lincc/fellowship_program).\n",
        "\n",
        "[![Open this notebook in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jwuphysics/LSSTC-DSFP-Session-19/blob/main/day3/Convolutional%20Neural%20Networks.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su957qcbyehk"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, we will:\n",
        "0. immediately train a convolutional neural network (CNN) regression model using a top-down approach,\n",
        "1. build up intuition for how convolution layers work,\n",
        "2. learn about some of the other ingredients in CNNs,\n",
        "3. examine the forward and backward passes of the CNN model,\n",
        "4. implement a full training loop.\n",
        "\n",
        "As a bonus objective, we will also implmement a CNN classifier on a different data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB51WX_vWck7"
      },
      "source": [
        "## Before getting started\n",
        "\n",
        "- **Open this notebook in Colab.** Click the above \"badge\" to open this notebook in Google Colab. This will expedite the setup process and allow you to train a model using a GPU (if available).\n",
        "\n",
        "- **Use a GPU runtime.** This will make training neural networks much faster (but it will restart your runtime). You can do this by clicking on:\n",
        "> Runtime > Change runtime type > Hardware accelerator - *select GPU and save*\n",
        "\n",
        "- **Install some packages.** We will use `fastai` which is built on top `pytorch`, a popular deep learning framework. Unfortunately, these libraries have other dependencies (like CUDA, the language that the GPU speaks) and can take a while to install, but most of them should already be available on Colab.\n",
        "\n",
        "- **Download image files.** Using convolutional neural networks, we can learn directly from individual pixels in multi-channel images. We will be using Sloan Digital Sky Survey $gri$-band image cutouts in the JPG format (see e.g. [SDSS SkyServer](http://skyserver.sdss.org/dr16/en/help/docs/api.aspx#imgcutout) or [DESI Legacy Imaging Surveys](https://www.legacysurvey.org/viewer/urls)).\n",
        "\n",
        "- **Download value-added catalogs.** We will be predicting the gas-phase metallicity $Z$ for each galaxy. These metallicities are estimated from a model based on optical-wavelength strong lines ([Tremonti et al. 2004](https://ui.adsabs.harvard.edu/abs/2004ApJ...613..898T/abstract))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FM5SpZrEYE6q"
      },
      "outputs": [],
      "source": [
        "# @title Install and import `fastai`\n",
        "# fastai is a high-level deep learning library built on Pytorch\n",
        "!pip install -q fastai --upgrade\n",
        "\n",
        "# this also implicitly imports numpy, pandas, etc\n",
        "from fastai.basics import *\n",
        "from fastai.vision.all import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yPmBWxLVfdIc"
      },
      "outputs": [],
      "source": [
        "# @title Download SDSS catalog and images\n",
        "# download catalog of ~14000 galaxies\n",
        "!wget -q -nc -O galaxies.csv https://www.dropbox.com/s/e5rfeg71ub8ab0k/galaxies.csv?dl=0\n",
        "\n",
        "# download corresponding image cutouts\n",
        "!wget -q -nc -O images.tar.gz https://www.dropbox.com/s/ru3cgknmidn0re9/images.tar.gz?dl=0\n",
        "!tar xzf images.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJIf50G5W2_i",
        "outputId": "6e5e6313-d6c1-40a4-bd97-7053d873fa99"
      },
      "outputs": [],
      "source": [
        "!ls --color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCmHNDoqgaRL"
      },
      "source": [
        "Our directory structure should look something like this:\n",
        "\n",
        "```\n",
        ".\n",
        "├── galaxies.csv\n",
        "├── images\n",
        "│   ├── 1237648672922534053.jpg\n",
        "│   ├── 1237648673459667002.jpg\n",
        "│   └── [...]\n",
        "└── [...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvSt-NBhf3jI",
        "outputId": "3148d23a-c242-457a-92e9-f8273c5924f3"
      },
      "outputs": [],
      "source": [
        "# you can check your GPU using this command:\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXuVrDZMf6zb"
      },
      "source": [
        "If you see a message like\n",
        "> NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
        "\n",
        "then make sure you change your Runtime/Hardware accelerator to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBEFuyHogJOR"
      },
      "source": [
        "## Can we predict a galaxy's gas-phase metallicity using three-color images?\n",
        "\n",
        "[Acquaviva (2016)](https://ui.adsabs.harvard.edu/abs/2016MNRAS.456.1618A/abstract) found that $ugriz$ magnitudes and colors, plus stellar mass and photometric redshifts, supplied enough information to predict $Z$ to within a root mean squared error (RMSE) of $\\sim 0.08$. But this requires another model to estimate stellar masses, photometric redshifts, and extract the magnitudes! Can we estimate metallicity purely using images?\n",
        "\n",
        "The answer turns out to be yes! [Wu & Boada (2019)](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.4683W/abstract) also achieved RMSE$\\ < 0.09$ scatter in predicting metallicity solely using $gri$-band imaging!\n",
        "\n",
        "Okay cool, let's go ahead and do it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPUofyvijEnP"
      },
      "source": [
        "## But first, we want to take a look at our data.\n",
        "\n",
        "Note that I've already selected a clean and small subsample from the entire SDSS Main Galaxy Survey, which is spectroscopically complete for galaxies in the SDSS footprint with $r < 17.78$ (modulo fiber collisions).\n",
        "\n",
        "However my sub-selection process will *bias* the sample a bit, so our results today won't be representative of the full galaxy population (and will differ from the Wu & Boada 2019 results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "upXESrM8_aP9",
        "outputId": "d1e552b6-c580-4c08-b989-3dafcdbb4cc1"
      },
      "outputs": [],
      "source": [
        "# @title Examine the value-added catalog, including metallicity\n",
        "\n",
        "df = pd.read_csv('galaxies.csv', dtype={'objID': str})\n",
        "n_galaxies = len(df)\n",
        "\n",
        "# set a random state\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "df.sample(5, random_state=rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_yu1s_qiHnJ"
      },
      "source": [
        "Note that metallicity is defined $Z = 12 + \\log({\\rm O/H})$, and so the `oh_p50` is the column name for the 50th percentile estimate of the model-based metallicity. There are also a few other columns: Right Ascension (`ra`), Declination (`dec`), redshift (`z`), logarithmic stellar mass in solar masses (`lgm_tot_p50`), and star formation rate (`sfr_tot_p50`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "74yiWr5qjqCf",
        "outputId": "133409b2-99c5-4968-c543-8ce0057a8aa6"
      },
      "outputs": [],
      "source": [
        "# @title Plot a histogram of metallicities\n",
        "plt.figure(figsize=(6, 3), dpi=150)\n",
        "plt.hist(df.oh_p50, bins=25, color='#ff6361')\n",
        "plt.xlabel('$Z$ (gas metallicity)', fontsize=16)\n",
        "plt.ylabel('Number', fontsize=16)\n",
        "plt.grid(alpha=0.15);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "ZT9OOVR9hdRg",
        "outputId": "76655a0f-4369-419e-8dda-09c6e7187a60"
      },
      "outputs": [],
      "source": [
        "# @title View six random galaxies and display their metallicities\n",
        "fig, axes = plt.subplots(2, 3, figsize=(8, 6), dpi=150)\n",
        "for i, ax in zip(rng.choice(range(n_galaxies), size=6, replace=False), axes.flat):\n",
        "\n",
        "    objID = df.objID.iloc[i]\n",
        "    PILImage.create(f'images/{objID}.jpg').show(ax=ax, title=objID)\n",
        "\n",
        "    Z_gas = df.oh_p50.iloc[i]\n",
        "    M_star = df.lgm_tot_p50.iloc[i]\n",
        "    ax.text(0.5, 0.85, f'$Z$ = {Z_gas:.3f}', ha='center', transform=ax.transAxes, size=16, color='white')\n",
        "\n",
        "fig.subplots_adjust(wspace=0.05, hspace=0.07)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSem5nhlj7GT"
      },
      "source": [
        "## Training a neural network with fastai\n",
        "\n",
        "The **top-down** approach: let's solve this deep learning problem in ~20 lines of code, split across the next three cells!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "qYWoJfa__nk8",
        "outputId": "e9b69584-9d7f-4702-8dbd-e057011377cb"
      },
      "outputs": [],
      "source": [
        "# fastai \"data blocks\" determine how data can be fed into a model\n",
        "dblock = DataBlock(\n",
        "    blocks=(ImageBlock, RegressionBlock),\n",
        "    get_x=ColReader('objID', pref='images/', suff='.jpg'),\n",
        "    get_y=ColReader('oh_p50'),\n",
        "    splitter=RandomSplitter(0.2, seed=42),\n",
        "    item_tfms=[Resize(160), CropPad(144)],\n",
        "    batch_tfms=aug_transforms(do_flip=True, flip_vert=True, max_rotate=0, max_zoom=1.0, max_warp=0, p_lighting=0) + [Normalize()]\n",
        ")\n",
        "\n",
        "# \"data loaders\" actually load the data\n",
        "dls = ImageDataLoaders.from_dblock(dblock, df, bs=128, device=\"cuda\")\n",
        "dls.show_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLYzusLJAt3B"
      },
      "outputs": [],
      "source": [
        "# root mean squared error is our loss function\n",
        "def RMSE(p, y): return torch.sqrt(MSELossFlat()(p, y))\n",
        "\n",
        "# this is a reasonably deep CNN model with some bells and whistles\n",
        "cnn_model = xresnet18(n_out=1, sa=True, act_cls=Mish)\n",
        "\n",
        "# a \"learner\" evaluates and updates the model\n",
        "learn = Learner(dls, cnn_model, loss_func=RMSE, opt_func=Adam).to_fp16()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "GMcQ_D32BtHA",
        "outputId": "7d9a058b-dff4-4989-fd6f-c060180b3e8f"
      },
      "outputs": [],
      "source": [
        "# train in under ten minutes on NVidia T4\n",
        "learn.fit_one_cycle(10, 1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRDMCy-f4gCW"
      },
      "source": [
        "Note: your training results may vary, but hopefully not by too much!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "g1WpbaAt4nFk",
        "outputId": "82f879f4-3e83-449c-c8cc-4684a2f180f8"
      },
      "outputs": [],
      "source": [
        "# @title Show loss curves\n",
        "fig = plt.figure(figsize=(6, 3), dpi=150)\n",
        "\n",
        "learn.recorder.plot_loss(skip_start=200)\n",
        "\n",
        "plt.xlabel('Iteration', fontsize=16)\n",
        "plt.ylabel('RMSE Loss', fontsize=16)\n",
        "plt.ylim(0.08, 0.3)\n",
        "plt.grid(alpha=0.15);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHr9ACEF8pT"
      },
      "source": [
        "## Get and evaluate validation set predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6sWtI5cQGBKs",
        "outputId": "b51555f7-84ec-4131-bb2b-869dff2225d3"
      },
      "outputs": [],
      "source": [
        "Z_pred, Z_true = learn.get_preds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "8xNEhP_qGIVp",
        "outputId": "2e232278-5c80-46ca-c8e0-34fecd5b0549"
      },
      "outputs": [],
      "source": [
        "# @title Show metallicity validation results in a scatter plot\n",
        "plt.figure(figsize=(6, 6), dpi=150)\n",
        "\n",
        "plt.scatter(\n",
        "    Z_true.detach().cpu().unsqueeze(0),\n",
        "    Z_pred.detach().cpu(),\n",
        "    c='#003f5c',\n",
        "    edgecolor='none',\n",
        "    s=5\n",
        ")\n",
        "\n",
        "plt.plot([8, 9.5], [8, 9.5], c='w', lw=3, alpha=1, zorder=8)\n",
        "plt.plot([8, 9.5], [8, 9.5], c='0.8', lw=1, alpha=1, zorder=9)\n",
        "\n",
        "plt.grid(alpha=0.15)\n",
        "plt.xlim(8.2, 9.4)\n",
        "plt.ylim(8.2, 9.4)\n",
        "plt.xlabel('True metallicity', fontsize=16)\n",
        "plt.ylabel('Predicted metallicity', fontsize=16);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJA2JsofqzbW"
      },
      "source": [
        "# Understanding convolutions\n",
        "\n",
        "A picture tells a thousand words, and an animation tells... a lot of pictures?\n",
        "\n",
        "<!-- ![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif) -->\n",
        "![](http://media5.datahacker.rs/2020/12/movie1-1-1.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvLuNsoLrh3f"
      },
      "source": [
        "<details>\n",
        "<summary>A technical aside</summary>\n",
        "Convolutions can also be computed as matrix multiplications.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*9ngOwG-uHaJO8Od0ePB-fQ.jpeg\">\n",
        "\n",
        "This is an important computational point, and highlights why GPUs have accelerated deep learning tremendously in the past decade. GPUs are capable of running many matrix multiplication operations in parallel, so convolutions (as well as other neural network components) can be run extremely quickly.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4GvoHwaY8QC"
      },
      "source": [
        "## A sequence of convolutional layers\n",
        "\n",
        "![Credit: https://www.analyticsvidhya.com/blog/2020/10/what-is-the-convolutional-neural-network-architecture/](https://editor.analyticsvidhya.com/uploads/90650dnn2.jpeg)\n",
        "\n",
        "Just as we can think about images in the RGB channels, or astronomical images with $F606W$ + $F814W$ channels, which are *filtering color information* in the spectrum of galaxies based on their stellar or nebular gas content or whatever, we can think of a convolutional *layer* as containing multiple *filters* that each correspond to different pieces of information. Each filter tries to search out some combination of color and spatial morphology -- like edges, wavelet patterns, etc.\n",
        "\n",
        "The next layer of the CNN then acts on top of these outputs, or *activation maps*. That is, they take the previous layer's output as input, and then search for higher-level (or more complex) morphological features. For example, multiple edges at different angles can be used to detect curves, and different kinds of wavelets might be combined to identify textured patterns.\n",
        "\n",
        "In functional form, a neural network might look something like this:\n",
        "\n",
        "$$ {\\rm CNN}(x) = {\\rm Layer_{\\rm final}} \\Big [ \\cdots \\Big ( {\\rm Layer_2} \\big ( {\\rm Layer}_1(x) \\big ) \\Big) \\Big ], $$\n",
        "\n",
        "where each layer might contain a convolutional layer and other ingredients (see section below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbZiJf0pc1Hy"
      },
      "source": [
        "## Visualizing learned convolutional filters\n",
        "A particularly useful way to investigate what a trained CNN has learned is by looking at the kinds of images that are \"activated\" -- or produce high-valued outputs -- for different convolutional filters. [Zeiler & Fergus (2013)](https://arxiv.org/abs/1311.2901) wrote a paper looking into the kinds of filters that a CNN learned from a popular data set called ImageNet.\n",
        "\n",
        "![](https://jithinjk.github.io/blog/images/loss_visualize/12.png)\n",
        "![](https://jithinjk.github.io/blog/images/loss_visualize/3.png)\n",
        "![](https://jithinjk.github.io/blog/images/loss_visualize/45.png)\n",
        "\n",
        "In layers 2-5, you can see the filters (left) and the kinds of images that activated those filters (right).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePYGmtPQedPG"
      },
      "source": [
        "## Demos\n",
        "\n",
        "To get a feel for how multiple convolutional layers work, try out these demos:\n",
        "- [MNIST handwritten digit classifier](http://www.cs.cmu.edu/~aharley/vis/conv/flat.html)\n",
        "- [CNN in a spreadsheet!](https://docs.google.com/spreadsheets/d/1FynckVJJbCzpu8RVRxGPKtnlCPYtDnam3vZGYdbcKB4/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h3_Y1UKw1EL"
      },
      "source": [
        "# Other neural network ingredients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z_-5AE6w4-4"
      },
      "source": [
        "## Activation functions\n",
        "It turns out that we can't simply chain together multiple convolutional layers. Why? Well the rules of linear algebra state that the product of any number of matrices can be represented by yet another matrix. If you want something that's not represented by just a single matrix, you need a **non-linear function** in between neural network layers.\n",
        "\n",
        "These are sometimes called **activation functions** because of the way that we expect neurons to get activated.\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2018/01/Neuron_Hand-tuned.png)\n",
        "![](https://www.jeremyjordan.me/content/images/2018/01/single_neuron.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpZU56dzeS-5"
      },
      "source": [
        "## Batch Normalization\n",
        "\n",
        "Due to memory constraints, we need to feed in images one \"batch\" at a time. The number of images is often called the \"batch size\". Let's suppose we use a batch size of 64.\n",
        "\n",
        "Images flow through the network one batch at a time, so the inputs should be arrays of size $(N_{\\rm batch\\ size}, N_{\\rm filters}, N_{\\rm height}, N_{\\rm width})$. In other words, a batch of three-color $160 \\times 160$ images has the array shape $(64, 3, 160, 160)$.\n",
        "\n",
        "*Batch normalization* (batchnorm) is a mechanism that keeps the distribution of activations close to a Gaussian distribution. It does this by subtracting the mean and dividing by the standard deviation of its inputs, modulated by two additional hyperparameters. The batchnorm layer is usually inserted before the activation function layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7XoM165Qylw"
      },
      "source": [
        "## Pooling Layers\n",
        "\n",
        "Later throughout the CNN, more filters (aka channels) are needed to represent the different kinds of morphological features that might be seen in an image. However, the size of the data that flows through is still of shape $(N_{\\rm batch\\ size}, N_{\\rm filters}, N_{\\rm height}, N_{\\rm width})$, and we still need to multiply these by the parameters of the neural network!\n",
        "\n",
        "To lessen the computational burden, we rely on pooling layers: *max pooling* and *mean pooling* are commonly seen. If you double the number of filters, but halve the height and width of the output image by using max pooling with a $2\\times 2$ kernel, then you still have fewer outputs than from the preceding layer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mep8Nrfcw-U-"
      },
      "source": [
        "## Fully connected layers\n",
        "\n",
        "What happens towards the end of a CNN, after all of the convolutional layers? We have a bunch of activation maps, let's say, 512 of these activation maps which are each $7 \\times 7$ pixels. We can *flatten* them into 1-d vectors with $512 \\times 7 \\times 7 = 25,088$ elements. (*Note. This is just an example number.*)\n",
        "\n",
        "These 25,088 numbers can be used to describe any galaxy image, and you might think of them as some complicated, high-dimensional, feature vector. They can be fed into another in order to predict the metallicity -- just as we might use some photometric feature vector containing magnitudes, half-light radius, concentration index, etc.\n",
        "\n",
        "To link our CNN features to the final output, we'll use one or more *fully connected* layers (aka *dense* or *linear* layers). If you're confused about the name, just remember that we could represent convolutions with *sparse* matrix multiplications; now, we are performing dense matrix multiplication.\n",
        "\n",
        "We might want to consolidate our 25,088 numbers into a smaller number (let's go with 512) of features, and then finally use those 512 features to directly predict a single quantity: $Z$. These can each be represented as matrix multiplication using fully connected layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ath1r_hdxDto"
      },
      "source": [
        "## The forward pass\n",
        "\n",
        "So far we've only been describing the forward pass of the model. Basically, we can assume that we have a model that takes input images, and then outputs a single number (per image): the metallicity.\n",
        "\n",
        "This can be done even if the model isn't trained yet. That just means that the outputs turn out to be rather bad predictions. In the section below (Optimization), we will see how these outputs can be compared to the targets (aka labels), and then we can optimize the model in order to make it better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPPaJiQeyRX5"
      },
      "source": [
        "# A simple CNN forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvsbNZVhdjYe"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # the first convolutional layer, followed by batch normalization,\n",
        "        # and then the ReLU activation function\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # second convolutional layer has a similar sequence\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # the final fully connected layer is a bit different\n",
        "        self.fc = nn.Sequential(\n",
        "            AdaptiveAvgPool(4),   # creates a 64x4x4 mean pooling layer\n",
        "            nn.Flatten(),         # unravels the outputs\n",
        "            nn.Linear(64*4*4, 1)  # just a matrix multiplication\n",
        "        )\n",
        "\n",
        "    # this is called whenever we call the model directly\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvnJCmGrjyYH",
        "outputId": "016fe924-c5bb-475b-d2bb-17d731d3dbb7"
      },
      "outputs": [],
      "source": [
        "# create an instance on CPU\n",
        "model = SimpleNet()\n",
        "\n",
        "# send to GPU\n",
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIhM3J3zj7I7"
      },
      "outputs": [],
      "source": [
        "# evaluation mode means that we won't be updating the model, and therefore\n",
        "# don't need to keep track of how the model was incorrect.\n",
        "m = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsvbo8zVkTS8"
      },
      "source": [
        "Let's feed it some data! We can grab a single batch using `dls.one_batch()`. This provides\n",
        "- `xb`, a batch of images of size ($N_{\\rm batch\\ size}, N_{\\rm channels}, N_{\\rm height}, N_{\\rm width}$)\n",
        "- `yb`, a batch of true metallicities of size ($N_{\\rm batch\\ size}$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClfNCnL6kI4G",
        "outputId": "b0f79777-1bc7-44b8-f990-a23b630452a9"
      },
      "outputs": [],
      "source": [
        "# a batch of data (note that fastai has placed these on the GPU for us already)\n",
        "xb, yb = dls.one_batch()\n",
        "\n",
        "xb.shape, yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jthuL1XlJ9_",
        "outputId": "102358e7-b23d-4155-e660-bde85f768fab"
      },
      "outputs": [],
      "source": [
        "# feed through first layer\n",
        "after_first_layer = m.conv1(xb)\n",
        "after_first_layer.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySgpVFe0l0ha",
        "outputId": "1483f500-9cf4-4dbc-f309-38f4e8108c41"
      },
      "outputs": [],
      "source": [
        "# feed through second layer\n",
        "after_second_layer = m.conv2(after_first_layer)\n",
        "after_second_layer.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh92QWtvmUos",
        "outputId": "4470b703-07fb-4603-eee1-7df9612cacbc"
      },
      "outputs": [],
      "source": [
        "# feed through first part of the fully connected sequence\n",
        "after_averagepool = m.fc[0](after_second_layer)\n",
        "after_averagepool.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7KhIE7nmlSo",
        "outputId": "d4a39e69-f671-454d-bac2-1e0fc9bf12ea"
      },
      "outputs": [],
      "source": [
        "# flattening simply changes the shape from 64x4x4 -> 1024\n",
        "after_flatten = m.fc[1](after_averagepool)\n",
        "after_flatten.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8icElELmpwA",
        "outputId": "68b84bad-71d4-4802-c52e-22ff9a95789d"
      },
      "outputs": [],
      "source": [
        "# finally perform a matrix multiplication (linear layer)\n",
        "after_linear = m.fc[2](after_flatten)\n",
        "after_linear.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzJdwTodnG9V"
      },
      "outputs": [],
      "source": [
        "# move from GPU back to CPU, and \"detach\" -- i.e., don't automatically\n",
        "# compute gradients (we'll get to this later)\n",
        "predictions = after_linear.detach().cpu().flatten()\n",
        "labels = yb.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "0sBWpEF8m8Av",
        "outputId": "80f43b88-e58d-41d1-b8e0-53821cd93a7a"
      },
      "outputs": [],
      "source": [
        "# compare outputs with ground truths\n",
        "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(8, 3.5), dpi=150)\n",
        "\n",
        "ax1.hist(predictions, bins=40, color='#003f5c')\n",
        "ax1.set_xlabel('$Z_{\\\\rm pred}$', fontsize=16)\n",
        "\n",
        "ax2.hist(labels, bins=40, color='#ff6361')\n",
        "ax2.set_xlabel('$Z_{\\\\rm true}$', fontsize=16);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH8URqlfI8JT"
      },
      "source": [
        "Note that these are very different! The typical prediction (before any CNN optimization) returns a value close to zero. This is unsurprising, as repeated matrix multplications will generally drive the outputs toward zero or infinity. Clever initialization of the neural network parameters can help us avoid divergence at first, but we will still need to optimize this network in order to get in the correct ballpark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A3ivsrkeLth"
      },
      "source": [
        "## **Exercise 1**\n",
        "\n",
        "**The input image is $160 \\times 160$ pixels, but after the first convolution layer, the size of each activation channel is $80 \\times 80$, and after the second convolution layer, the size of each activation channel is $40 \\times 40$. Why does it get halved each time?**\n",
        "\n",
        "*Hint*: You can reference the [`torch.nn.Conv2d()` documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), and see what value we used for the `stride` parameter when creating the `SimpleNet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm6SWjD3fN2N"
      },
      "source": [
        "<details>\n",
        "<summary>Answer</summary>\n",
        "We used `stride=2`, which means that the convolution skips every other pixel. This is okay, as the `kernel_size` is set to 3, which ensures that every pixel is covered by a convolution kernel (although a convolution kernel will not be *centered* on every pixel).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlLHflGqfxO4"
      },
      "source": [
        "## **Exercise 2**\n",
        "\n",
        "**In the `SimpleNet`, we used a convenience function called `torch.nn.AdaptiveAvgPool2d()` in order to reduce the input from a $40 \\times 40$ map down to a $4 \\times 4$ map. How could we accomplish the same using [`torch.nn.AvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTbqWD4EhEZk"
      },
      "source": [
        "<details>\n",
        "<summary>Answer</summary>\n",
        "We would swap out `AdaptiveAvgPool2d(4)` with `nn.AvgPool2d(kernel_size=10)`.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i787jWpmxO4B"
      },
      "source": [
        "# Optimization, i.e., the backward pass\n",
        "\n",
        "Great, we now know how to pass images forward through a model, and get some garbage-quality outputs.\n",
        "\n",
        "But how do we make our model better? The whole point of machine learning is to let the model learn from examples, and eventually give you a better predictions!\n",
        "\n",
        "Other resources:\n",
        "- Overview of gradient optimization algorithms: https://ruder.io/optimizing-gradient-descent/\n",
        "- Mathematical deep dive into backpropagating the loss function: https://brilliant.org/wiki/backpropagation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hluPTFG3xdWm"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "In order to understand what it means for a model to do better, we first need to define what \"better\" means. In this case (regression on a single target), it's very easy. We've already talked about a loss function: the **root mean squared error (RMSE)**.\n",
        "\n",
        "We want to minimize the RMSE as a function of the model parameters, given the data examples. Whenever we perform a minimization of a function $J$, given the data $x$, we simply need to find the values of model parameters $\\theta$ that set its derivative to zero: $\\nabla_\\theta J(\\theta; x) = 0$.\n",
        "\n",
        "In practice, optimizing the loss function can be quite difficult. Neural networks are basically never convex functions, which means that you may not find a good solution (depending on your starting point). The loss function forms a high-dimensional surface, which I like to call a loss landscape -- this landscape might be riddled with saddle points and bad local minima, and so you need to find a way to iteratively update your model such that it gives better predictions.\n",
        "\n",
        "If you want to see a glimpse of the kinds of topologies that these loss landscapes create, check out [this video](https://www.youtube.com/watch?v=As9rW6wtrYk) or the image below:\n",
        "<img src=\"https://www.cs.umd.edu/~tomg/img/landscapes/noshort.png\" width=\"500\" height=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe9zYKFoxgrr"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "The gradient, $\\nabla_\\theta J(\\theta; x)$, points in the direction (in our model parameter space) that corresponds to lower loss. That is, it tells us better values of our model parameters $\\theta$ in order to improve predictions. This method is called **gradient descent**.\n",
        "\n",
        "There's just a few problems. First, we'll be sampling different batches of data at a time, and the loss landscape actually depends on which batch of data $x$ we use! So the loss landscape is constantly shifting! (Note that this means we're actually doing *stochastic* gradient descent.) Second, computing the gradient only tells us the *local* direction of steepest descent, but it doesn't actually illustrate the entire loss landscape. If it's like descending down a mountain top, all we know is our current altitude (i.e. our loss), and the direction where we may want to go; we can take a step in that direction and then check our altitude after that step.\n",
        "\n",
        "The smoothness of our loss landscape matters a lot here. If it's super bumpy, then the direction of steepest descent might vary wildly. This is a challenging problem for optmization. One of the ways to smooth the trajectory of our steps is by using an **adaptive** approach (see section on *Adam* below).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYCN3opzxqJ3"
      },
      "source": [
        "## Backpropagation of error\n",
        "\n",
        "A third problem might not be immediately obvious: our model parameters actually depend *on each other*. If we want to update our model in the final fully connected layer of the neural network, then it also means that we'll have to tweak model parameters in the previous layer, which then connect to the model parameters in the layer before, and etc. Fortunately, this is computationally tractable and done automatically by `Pytorch`. Indeed, propagating the gradient is just called the *chain rule for derivatives*.\n",
        "\n",
        "`Pytorch` provides everything we need for computing gradients throughout the model. Even making updates to the model is easy:\n",
        "\n",
        "$$ \\theta_{i+1} = \\theta_i - \\alpha \\nabla_{\\theta{_i}} J_i,$$\n",
        "\n",
        "where $\\alpha$ is the step size in the model parameter space, or **learning rate**, but we will discuss that in a section  below. $J$ is the RMSE loss function (in this case). Note that we use a negative sign, because we're trying to minimize the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IktZGv2yF9S"
      },
      "source": [
        "## Preventing overfitting and improving optimization\n",
        "\n",
        "After a large number of updates, we might find that our gradient descent algorithm is able to approach the minimum of the loss surface. Remember that we don't necessarily the *global minimum* here: that is impossible because our surface constantly changes with different batches of data, and besides, we might start overfitting.\n",
        "\n",
        "There are lots of ways, qualitatively, to prevent overfitting. One is to design a model that smooths over all the sharp edges in the model. For example, a popular CNN architecture is called the [ResNet architecture](https://arxiv.org/abs/1512.03385), and it introduces \"skip connections\" that make the loss landscape much nicer (and neural network optimization much, much better; Figure from [this paper](https://arxiv.org/abs/1712.09913)):\n",
        "![](https://jithinjk.github.io/blog/images/loss_visualize/loss_1.png)\n",
        "\n",
        "There are lots of other methods for preventing overfitting. One method called **Dropout** borrows from the idea of random forests: while any highly optimized model might be prone to overfit, a random ensemble of slightly less optimized models might be a lot better for generalization. It does this by randomly setting neurons in the final layers of the CNN to zero, which forces the network to learn redundant pathways for arriving at a good prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYie7dj3yq5f"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "One way to improve generalization in our image data set is to leverage symmetries. For example, horizontal and vertical flips, and rotations of 90, 180, and 270 degrees can improve the number of images by a factor of eight! (This is called the $D_4$ dihedral group, in case you care about group theory...)\n",
        "\n",
        "There are other ways to \"augment\" our data set, for example, by also leveraging (approximate) translational invariance, but we don't do that here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6RmvundEh8"
      },
      "source": [
        "# The simple CNN model in action (forward + backward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQOnto7GhE4d"
      },
      "source": [
        "## Load data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9vyBPXsdHcj"
      },
      "outputs": [],
      "source": [
        "# re-initialize our simple model\n",
        "model = SimpleNet()\n",
        "model.to('cuda');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRRp-9_GdyXU"
      },
      "outputs": [],
      "source": [
        "# the oppposite of \"eval\" model is \"train mode\" -- let's ensure we're in\n",
        "# training mode, where Pytorch will keep track of gradients\n",
        "model.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaR3igaEeWR7"
      },
      "outputs": [],
      "source": [
        "# initialize the loss\n",
        "train_loss = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0ZFyU3me3hH",
        "outputId": "df9295bb-8676-41e6-c2ab-0c624dc879e7"
      },
      "outputs": [],
      "source": [
        "# get the first (or next) batch\n",
        "xb, yb = next(iter(dls.train))\n",
        "\n",
        "xb.shape, yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrKy-Ypve6dx",
        "outputId": "3e5fa61c-2fa3-4b38-ce7d-fcf1c426935c"
      },
      "outputs": [],
      "source": [
        "# this should already be on the GPU\n",
        "xb.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv8p7HRNhIBT"
      },
      "source": [
        "## The forward pass (again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM-tIAgsfKXE",
        "outputId": "6641cdc2-5c02-4348-c1ab-2793d48bb9db"
      },
      "outputs": [],
      "source": [
        "# make a (very bad) prediction as before\n",
        "prediction = model(xb)\n",
        "\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sORX48KflMp"
      },
      "outputs": [],
      "source": [
        "# define our loss function\n",
        "def RMSE(p, y):\n",
        "    \"\"\"Square root of the flattened mean squared error loss\"\"\"\n",
        "    return torch.sqrt(MSELossFlat()(p, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjtS3hgFgBct",
        "outputId": "56a03c9c-d451-4349-f985-0318bdb3da86"
      },
      "outputs": [],
      "source": [
        "# evaluate the loss\n",
        "loss = RMSE(prediction, yb)\n",
        "\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsycA8S1KGc4"
      },
      "source": [
        "Remember how our true metallicities were centered around $Z_{\\rm true} \\approx 9$, while our initial predictions from the untrained CNN were close to 0? That's why the typical loss is 9, give or take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3mWRgW7gGzc"
      },
      "outputs": [],
      "source": [
        "# keep track of the loss\n",
        "train_loss += loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ44qJMshLNr"
      },
      "source": [
        "## The backward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a3De9f2gMEg"
      },
      "outputs": [],
      "source": [
        "# initialize our optimizer (we'll use stochastic gradient descent for now)\n",
        "# - it needs to be passed the model parameters in order to update them\n",
        "# - we will use a value of 0.01 for the learning rate, but we'll discuss this more later\n",
        "optimizer = SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "107SnXdpgncZ"
      },
      "outputs": [],
      "source": [
        "# remove any previous gradients\n",
        "optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4BO8bH_g5yq"
      },
      "outputs": [],
      "source": [
        "# perform the backwards pass (and thus compute all gradients -- thanks Pytorch!)\n",
        "# note that gradients are attached to the model parameters\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8tzJGLWg9lF"
      },
      "outputs": [],
      "source": [
        "# update model parameters\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkTrW2PihR6s"
      },
      "source": [
        "Yay! We've done a single update of our neural network! Usually we will keep passing training data through until we've fully completed a run through the entire training sample. Then, we will check how well we did on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1QDqv_fhVu5"
      },
      "source": [
        "## Putting it all together: one epoch of training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H-qh8TAdw-J",
        "outputId": "d634e328-5c9d-4c0d-a8e4-a7a198273803"
      },
      "outputs": [],
      "source": [
        "# initialize model, optimizer, etc.\n",
        "model = SimpleNet()\n",
        "model.to('cuda')\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# train\n",
        "# =====\n",
        "model.train()\n",
        "train_loss = 0\n",
        "\n",
        "# continue grabbing batches of training data\n",
        "for xb, yb in iter(dls.train):\n",
        "    prediction = model(xb)\n",
        "    loss = RMSE(prediction, yb)\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# validate\n",
        "# ========\n",
        "valid_loss = 0\n",
        "\n",
        "# no need to compute gradients, since we're not trying to update the model\n",
        "# this let's us perform another forward pass after one epoch of training\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for xb, yb in iter(dls.valid):\n",
        "        prediction = model(xb)\n",
        "        loss = RMSE(prediction, yb)\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "# see how well we did\n",
        "train_loss = train_loss / len(dls.train)\n",
        "valid_loss = valid_loss / len(dls.valid)\n",
        "\n",
        "print(f'Train: {train_loss:.4f}      Valid: {valid_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8j7zcHNkQH7"
      },
      "source": [
        "## **Exercise 3**\n",
        "\n",
        "**Now let's put it all together! Write a full training loop with 20 epochs of training.**\n",
        "- **At the end of each training epoch, iterate through batches of the validation loader (remember that we stored it in `dls.valid`) in order to determine the validation loss.**\n",
        "- **Make sure to also record the training and validation losses so that we can plot the loss curves later.**\n",
        "\n",
        "(Note: this is still pretty slow. Most of the cost is due to loading all the data from memory onto the GPU. It actually may be faster to run everything on a CPU for this part, but I haven't checked.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v264DUjyj6oc",
        "outputId": "6cc9d53a-01df-4e50-cebb-aef5b1346f02"
      },
      "outputs": [],
      "source": [
        "# @title Answer\n",
        "\n",
        "# keep track of losses\n",
        "train_losses = [train_loss]\n",
        "valid_losses = [valid_loss]\n",
        "\n",
        "# iterate through more training (20 additional epochs)\n",
        "for epoch in range(1, 21):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for xb, yb in iter(dls.train):\n",
        "        prediction = model(xb)\n",
        "        loss = RMSE(prediction, yb)\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for xb, yb in iter(dls.valid):\n",
        "            prediction = model(xb)\n",
        "            loss = RMSE(prediction, yb)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    # we typically normalize the loss by the number of batches\n",
        "    train_loss = train_loss / len(dls.train)\n",
        "    valid_loss = valid_loss / len(dls.valid)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'Epoch: {epoch:2d}    Train: {train_loss:.4f}      Valid: {valid_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "YnGPieVWk4CT",
        "outputId": "b611e19e-a73f-475f-d1fb-d45d2a00b3b5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3), dpi=150)\n",
        "plt.plot(range(0, 21), train_losses, c='#003f5c', lw=3, label='Train')\n",
        "plt.plot(range(0, 21), valid_losses, c='#ff6361', lw=3, label='Valid')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('RMSE Loss')\n",
        "\n",
        "plt.legend(framealpha=0, fontsize=14)\n",
        "plt.grid(alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-40fhbeR_d7"
      },
      "source": [
        "We can see that our simple CNN implementation and training procedure isn't too great; 20 epochs of training only results in a RMSE loss of about 0.35, whereas we previously achieved < 0.10 RMSE loss in 10 epochs of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYhnh3gbx7eh"
      },
      "source": [
        "# (Optional) Hyperparameters and expediting the optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2QdFhz6yAIj"
      },
      "source": [
        "## Learning rate\n",
        "\n",
        "The learning rate is the most important hyperparameter for *efficiently* training a neural network. It determines how large each update is -- you can think of it as the coefficient to the gradient that is computed from the loss function. For example, a 1-dimension loss landscape $J(\\theta)$ is shown below, while gradients multiplied by the learning rate are shown as red arrows:\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
        "\n",
        "We want to make sure that we select an optimal learning rate! There's a nice empirical method for doing this described by [Leslie Smith](https://arxiv.org/pdf/1506.01186.pdf) (and implemented in [Fastai](https://docs.fast.ai/callback.schedule.html#Learner.lr_find))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou8SoUguyBml"
      },
      "source": [
        "## Weight decay\n",
        "\n",
        "Regularization techniques mitigate overfitting by preventing the model from becoming overly dependent on just a few neurons in the network. Weight decay is a form of regularization related to (and sometimes misconstrued as) L2 regularization -- a method for penalizing extremely large neural network weights:\n",
        "\n",
        "$$ J(\\theta) = {\\rm RMSE} + \\frac{1}{2} \\lambda ||\\theta||^2,$$\n",
        "\n",
        "where $\\lambda$ is the weight decay coefficient and $\\theta$ are the values of the network's weights.\n",
        "\n",
        "Note that weight decay can sometimes clash with adaptive gradient descent methods (e.g., Adam, which is described below) and with batch normalization (introduced above). The respective solutions are to use [weight decay-decoupled Adam (AdamW)](](https://arxiv.org/abs/1711.05101)) and [to not use weight decay in any layer preceding batch normalization](](https://blog.janestreet.com/l2-regularization-and-batch-norm/)) (i.e., convolutional layers).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3AQhafAyaVU"
      },
      "source": [
        "## Gradient descent with Momentum and adaptive momentum (Adam)\n",
        "\n",
        "Stochastic gradient descent (SGD) is a nice straightforward method for maneuvering about the loss landscape and updating the neural network. A **momentum** term is commonly added: it simply keeps track of previous update directions, and includes a small contribution from those prior steps in the update rule:\n",
        "\n",
        "$$ \\mu_{i+1} = \\beta \\mu_i + \\nabla J(\\theta_i) $$\n",
        "$$ \\theta_{i+1} = \\theta_i - \\alpha \\mu_{i+1}$$\n",
        "\n",
        "If you take a look at many time steps, you'll find that the updates look like:\n",
        "\n",
        "$$ \\theta_{i+1} = \\theta_{i} - \\alpha \\Big [ \\nabla J(\\theta_{i})  + \\beta \\nabla J(\\theta_{i-1}) + \\beta^2 \\nabla J(\\theta_{i-2}) + \\beta^3 \\nabla J(\\theta_{i-3}) + \\cdots \\Big ]. $$\n",
        "\n",
        "The second term looks like the Taylor expansion of an exponential function, and that's what it is sometimes referred to as an *exponentially decaying average* or *exponentially-weighted moving average* (EMA) of the gradient. If you set the hyperparameter $\\beta$ to zero, we simply go back to gradient descent. A common choice for momentum is around $\\beta = 0.99$.\n",
        "\n",
        "A good way to gain intuition for how momentum works is by playing with [this awesome demo from distill.pub](https://distill.pub/2017/momentum/). The dynamics of adding momentum to gradient descent are like that of a ball rolling down a hill -- or loss landscape.\n",
        "\n",
        "**Adam** makes it so that the dynamics are like that of a ball *with friction* rolling down a hill. It can also be described as a simply modification of gradient descent, e.g., [here](https://ruder.io/optimizing-gradient-descent/index.html#adam). Adam has been marketed as a \"just plug it in and forget about it\" optimizer since it should adapt well to a variety of optimization problems (and while people have found notable instances where it doesn't work as well as SGD, the [original Adam paper](https://arxiv.org/abs/1412.6980) has managed to rack up >70k citations...)\n",
        "\n",
        "There are important caveats, e.g. [decoupling weight decay from the update step](https://arxiv.org/abs/1711.05101v3) which improves Adam significantly (sometimes this is called *AdamW*). This is incorporated in the Fastai `Adam()` implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptiXhQgzTWJo"
      },
      "source": [
        "## Learning rate schedules\n",
        "\n",
        "You may be wondering whether it makes sense to use a constant learning rate over the entire duration of training. Indeed, it doesn't make sense to take large steps towards the beginning of training when we're far away from a good model, and then continue taking large steps even as we're very close to an optimal region. Rather, we should decrease the learning rate over time, so as to make sure we are carefully descending into the local minima.\n",
        "\n",
        "This is often implemented by changing the learning rate according to some schedule (often with a functional form). The default schedule is constant over time. Early practioners often used a method of decreasing the learning rate by a factor of 10 every 30 epochs of training (i.e., once convergence is achieved at a given learning rate) --  this allows the network to continue fitting. There are more sophisticated learning rate schedules, e.g.,:\n",
        "- a decreasing function in the shape of a cosine (sometimes called cosine annealing)\n",
        "- a flat rate + cosine function\n",
        "- a function that first increases (called the warm-up phase) and then decreases (the annealing phase)\n",
        "\n",
        "Fastai implements the latter two methods, which have been shown to achieve [\"super-convergence\"](https://sgugger.github.io/the-1cycle-policy.html) -- extremely rapid convergence because of optimization at very high learning rates. These are implemented in the CNN `Learner` object as `fit_flat_cos()` and `fit_one_cycle()` functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaSsRVDSylmg"
      },
      "source": [
        "## Improving the CNN model\n",
        "\n",
        "We've already suggested that a CNN can benefit from batch normalization, skip connections, and a good choice of activation function. The original ResNet can be improved by adding [several small number of tweaks](https://arxiv.org/abs/1812.01187), and possibly by including other design choices. For example, the [Mish](https://arxiv.org/abs/1908.08681) or [SiLU](https://paperswithcode.com/method/silu) activation functions tend to produce more smooth loss landscapes than the oft-used ReLU (which is discontinuous in its first derivative). [Self-attention](https://arxiv.org/abs/1805.08318) can also be added after convolution layer activations to model *long-range* dependencies of morphological features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgPRiIPRdixx"
      },
      "source": [
        "## And that's it!\n",
        "\n",
        "Now that we've covered everything, go back to the top and re-run the code. There are some specifics about the Fastai `DataBlock` and `DataLoaders` objects that can be read about in their documentation, or in a [blog post here](https://jwuphysics.github.io/blog/galaxies/astrophysics/deep%20learning/computer%20vision/fastai/2020/05/26/training-a-deep-cnn.html#Organizing-the-data-using-the-fastai-DataBlock-API)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMnTedLoQXBJ"
      },
      "source": [
        "## **Exercise 4**\n",
        "\n",
        "**Can you get a RMSE below 0.09 dex in less than 10 training epochs? Experiment with various hyperparameter optimizations, for example:**\n",
        "- **Change the CNN architecture and/or modify activation, normalization, and attention layers**\n",
        "- **Use a different optimizer, regularizer, or loss function**\n",
        "- **Change the learning rate, batch size, or scheduler**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhFeOjfdEIW4"
      },
      "source": [
        "# Bonus! Hello Universe DeepMerge classification example\n",
        "\n",
        "See also the following resources...\n",
        "\n",
        "- [Hello Universe tutorial](https://archive.stsci.edu/hello-universe/deepmerge) (written in TensorFlow/Keras)\n",
        "- [MAST High Level Science Products](https://archive.stsci.edu/hlsp/deepmerge), which contain the nicely packaged data and documentation\n",
        "- [Ćiprijanović et al. 2020](https://ui.adsabs.harvard.edu/abs/2020A%26C....3200390C/abstract) (paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vuu9mZjFPCD"
      },
      "source": [
        "## Setting up the data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8soI0fA8EfNL"
      },
      "outputs": [],
      "source": [
        "from astropy.io import fits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elB6qsgmERr-"
      },
      "outputs": [],
      "source": [
        "!wget -q --no-clobber https://archive.stsci.edu/hlsps/deepmerge/hlsp_deepmerge_hst-jwst_acs-wfc3-nircam_illustris-z2_f814w-f160w-f356w_v1_sim-noisy.fits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdDvF6LUEbZW"
      },
      "outputs": [],
      "source": [
        "# organizing the data\n",
        "with fits.open('hlsp_deepmerge_hst-jwst_acs-wfc3-nircam_illustris-z2_f814w-f160w-f356w_v1_sim-noisy.fits') as h:\n",
        "    X_noisy = h[0].data\n",
        "    y_noisy = h[1].data\n",
        "\n",
        "X = tensor(np.asarray(X_noisy).astype('float32'))\n",
        "y = tensor(np.asarray(y_noisy).astype('float32'))\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKKdfvEpEerG"
      },
      "outputs": [],
      "source": [
        "# grabbing image statistics for normalization\n",
        "image_means = X_train[:100].mean(dim=(0, 2, 3))\n",
        "image_stds = X_train[:100].std(dim=(0, 2, 3))\n",
        "\n",
        "transforms_train = Pipeline([ToTensor, Dihedral, Normalize.from_stats(image_means, image_stds)])\n",
        "transforms_valid = Pipeline([ToTensor, Normalize.from_stats(image_means, image_stds)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI2VBzWRE4ZF"
      },
      "outputs": [],
      "source": [
        "# custom pytorch data set\n",
        "class GalaxyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = data\n",
        "        self.target = target.type(torch.LongTensor) # <-- make sure you cast this...\n",
        "        self.transform = transform\n",
        "\n",
        "        # needed for fastai\n",
        "        self.c = len(np.unique(target))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bggVwTZSFA3a"
      },
      "outputs": [],
      "source": [
        "dset_train = GalaxyDataset(X_train, y_train, transform=transforms_train)\n",
        "dset_valid = GalaxyDataset(X_valid, y_valid, transform=transforms_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8GLCRIYFEgH"
      },
      "outputs": [],
      "source": [
        "def norm(vals, vmin=None, vmax=None, Q=8, stretch=None):\n",
        "    \"\"\"\n",
        "    For visualization purposes normalize image with `arcsinh((vals-vmin)/(vmax-vmin)),\n",
        "    with vmin and vmax respectively defaulted to 0.01 and 0.99 quantiles of all values.\n",
        "\n",
        "    Q and stretch control the arcsinh softening parameter, see Lupton et al. 2004 and\n",
        "    see https://docs.astropy.org/en/stable/_modules/astropy/visualization/lupton_rgb.html#make_lupton_rgb\n",
        "    \"\"\"\n",
        "    vmin = vmin or np.quantile(vals, 0.02)\n",
        "    vmax = vmax or np.quantile(vals, 0.98)\n",
        "\n",
        "    if stretch is None:\n",
        "        return np.arcsinh(Q*(vals - vmin) / (vmax-vmin)) / Q\n",
        "    else:\n",
        "        return np.arcsinh(Q*(vals - vmin) / stretch) / Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "1ae4de93-8c6c-475e-a2ab-2593d018e44a",
        "outputId": "a17cd33b-5b5c-47c4-aff9-16eec6dede19"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8), dpi=100)\n",
        "\n",
        "for i, (image, cls) in enumerate(zip(*dset_train[:16])):\n",
        "    ax = fig.add_subplot(4, 4, i+1)\n",
        "\n",
        "    image = norm(image, Q=5).permute(1, 2, 0).clip(0, 1)\n",
        "\n",
        "    ax.imshow(image, aspect='equal')\n",
        "    ax.text(0.5, 0.85, \"Merger\" if cls == 1 else \"No merger\", ha='center', color='white', fontsize=16, transform=ax.transAxes)\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOrSZHy4FIjn"
      },
      "outputs": [],
      "source": [
        "# create dataloaders again\n",
        "dls = ImageDataLoaders.from_dsets(dset_train, dset_valid, device='cuda', bs=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSKdFG86F4ji"
      },
      "source": [
        "## **Exercise 5**\n",
        "\n",
        "**Setting up another CNN and train a *classifier* using `fastai`! Take note of the following:**\n",
        "- **Whereas before we were predicting one quantity, we now need to predict two quantities (merger versus non-merger probabilities). Adjust the final CNN layer accordingly.**\n",
        "- **Note that, for a classification problem, you will by default use a cross entropy loss between the binary categories and your prediction (which should be bounded between 0 and 1 via a softmax function). Fastai has a useful helper function that combines the softmax and cross entropy loss, called `CrossEntropyLossFlat()`.**\n",
        "- **Also keep track of the following metrics: `accuracy`, `Recall()`, and `Precision()`. What are recall and precision?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "vtvi4_4gn7wl",
        "outputId": "689b9477-fbda-41a9-e8f3-e9fe10e3cc3a"
      },
      "outputs": [],
      "source": [
        "# @title Answer\n",
        "model = xresnet18()\n",
        "model[-1] = nn.Linear(in_features=512, out_features=dls.c, bias=True)\n",
        "\n",
        "learn = Learner(\n",
        "    dls,\n",
        "    model,\n",
        "    loss_func=CrossEntropyLossFlat(),\n",
        "    opt_func=Adam,\n",
        "    metrics=[accuracy, Recall(), Precision()]\n",
        ")\n",
        "\n",
        "learn.cuda()\n",
        "\n",
        "learn.fit_one_cycle(10, 1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXiWcRtfFMod"
      },
      "source": [
        "We can usually achieve >70% accuracy in 10 epochs or so on the noisy dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9PCYSsnzK2k"
      },
      "source": [
        "## Next steps...\n",
        "\n",
        "Check out additional improvements to CNNs. For example, self-attention (used to great effect in large language models; [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)) can also be used in CNNs. Changing the activation function from ReLU to Leaky ReLU or a non-monotonic (e.g. GELU, Mish, etc) empirically seems to be useful. Adding deconvolution layers in place of batchnorm in the early portions of the network seems to improve performance ([Wu & Peek 2020](https://ui.adsabs.harvard.edu/abs/2020arXiv200912318W/abstract)). Some of these may provide the neural network architecture with inductive biases relevant to our astronomical data sets; feel free to explore more on these."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
