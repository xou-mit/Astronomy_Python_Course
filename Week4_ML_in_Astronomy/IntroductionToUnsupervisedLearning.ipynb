{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7ce0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b96353",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction to Unsupervised Learning\n",
    "\n",
    "##### Version 0.2\n",
    "\n",
    "***\n",
    "\n",
    "By AA Miller (Northwestern/CIERA)  \n",
    "07 September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47600cc9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Using the [ten hundred most common words](https://xkcd.com/simplewriter/) in the English language$^\\dagger$, I would describe unsupervised learning as\n",
    "\n",
    "> learning information without class names\n",
    "\n",
    "or \n",
    "\n",
    "> finding hidden breaks between groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68062a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "$^\\dagger$ See [Up-goer five](https://xkcd.com/1133/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80aec51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If we expand our volabulary slightly, then unsupervised learning could be described as \n",
    "\n",
    "> the search for underlying structure within the data\n",
    "\n",
    "or\n",
    "\n",
    "> learning from data without rewards (reinforcement learning) or labels (supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24aa262",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What follows is a broad overview of several of the common techniques that constitute unsupervised learning. \n",
    "\n",
    "We start with a familiar example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0415360",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "(you may not realize this, but you have all constructed unsupervised machine learning models...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e0c5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Suppose Nature produces some family of sources that tend to be either big or small. Mathematically we might represent the distribution of these sources as\n",
    "\n",
    "$$P \\propto 0.35\\mathcal{N}(30, 5^2) + 0.65\\mathcal{N}(60,8^2)$$\n",
    "\n",
    "where $\\mathcal{N}$ represents the normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec67177",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Using [`np.random`](https://numpy.org/doc/stable/reference/random/index.html) we can generate samples from this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc8593",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_samples(n_sources):\n",
    "    rand_dist = np.random.rand(n_sources)\n",
    "    size = np.empty_like(rand_dist)\n",
    "    small = rand_dist <= 0.35\n",
    "    size[small] = np.random.normal(30, 5, size=sum(small))\n",
    "    size[~small] = np.random.normal(60,8,  size=sum(~small))\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e599cbc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In practice, we observe nature and do not know the true probability density function (PDF) for the sources of interest.\n",
    "\n",
    "This brings us to our first example of unsupervised learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59605d98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1) Density Estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590a62d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### A) Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae5db9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If we measure the size of 100 sources from the aforementioned subject of interest, we can estimate the PDF via a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c958ad-bc21-4284-bde8-25986c7ca681",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(2016)\n",
    "sizes = generate_samples(100)\n",
    "\n",
    "def hist_plot(ax, fs=16):\n",
    "    ax.hist(sizes, histtype='step',lw=3)\n",
    "    ax.set_ylabel(\"N\", fontsize=fs)\n",
    "    ax.set_xlabel(\"size\", fontsize=fs)\n",
    "    ax.set_xlim(15,85)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75776ab5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "hist_plot(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a9de3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "I have previously discussed histograms, and their potential pitfalls, at length previously, so I will not subject you to that again. But I will quickly remind you that kernel density estimation (KDE) provides a nice alternative to histograms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcd3f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### B) KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fafdd-984c-4d11-b06d-712354478c89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kde_plot(ax, fs=15):\n",
    "    xgrid = np.linspace(0,100,5000)\n",
    "    kde = np.zeros_like(xgrid)\n",
    "    for s in sizes:\n",
    "        kern = 1/np.sqrt(2*np.pi*3**2)*np.exp(-(xgrid - s)**2/(2*3**2))\n",
    "        kde += kern\n",
    "        ax.plot(xgrid, kern, '0.5', lw=0.3, alpha=0.5)\n",
    "    ax.plot(xgrid, kde, 'k', lw=4)\n",
    "    ax.set_ylabel(\"N\", fontsize=fs)\n",
    "    ax.set_xlabel(\"size\", fontsize=fs)\n",
    "    ax.set_xlim(5,95)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb0c88",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kde_plot(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883cccc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A nice aspect of KDE is that it generalizes to multiple dimensions in a fairly straightforward fashion.\n",
    "\n",
    "Be aware of the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) for density estimation (and all unsupervised machine learning applications). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008597f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In brief, high dimensional datasets are inherently sparse. As the number of dimensions $D$ increases, all samples become relatively equidistant meaning clusters or regions of high density are difficult to identify.\n",
    "\n",
    "We can illustrate this by drawing 10 random points from a standard multivariate normal, and measure how the distance between the closest points increases with $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d514792",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "for n_dim in [2, 3, 10, 100, 1000]:\n",
    "    mean = np.zeros(n_dim)\n",
    "    cov = np.identity(n_dim)\n",
    "    samples = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "    min_sep = 1e8\n",
    "    for sample in samples:\n",
    "        sep = np.sqrt(np.sum((samples - sample)**2, axis=1))\n",
    "        min_sep = np.min([min_sep, np.min(sep[np.where(sep > 0)])])\n",
    "    print(f'For {n_dim} dimensions, min separation = {min_sep:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c333c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Fortunately, unsupervised learning can help with the curse of dimensionality via... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda77ce5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3cca3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dimensionality reduction is not the focus of today's lecture, but I will mention two methods that are common in the astrophysical literature. I will not present full derivations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9801733",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A) Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is the linear combination of variables that provide the maximum variability within a data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce67f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Using techniques from linear algebra (eigenvectors), PCA defines the direction of maximum variance (first principal component), then finds the direction of maximum variance that is orthogonal to the first component (second principal component), and so on.\n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"images/pca-3d.png\" width=\"450\" align=\"middle\">\n",
    "<div align=\"right\"> <font size=\"-3\">(credit: <a href=\"url\">https://learnopencv.com/principal-component-analysis/</a>) </font></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bbc1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "PCA has become less popular recently. It is scale dependent - if one feature varies from 0 to 1 and another varies from -500 to 500, the later will be determined to be more important. \n",
    "\n",
    "Furthermore, it is rarely the case that an orthogonal basis can truly describe all the variance present in a data set. Imagine a bunch of points on the surface of a sphere for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62071120-7a27-4882-a4af-7132dbda784d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "(this will be covered in more detail in Bryan's talk that follows this one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555be13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## B) t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d940b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "t-SNE is magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a4fdc5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The hand-waving explanation is that t-SNE finds a low-dimensional projection of high-dimensional data sets where small and large distances in the high-dimensional space are preserved in the low-dimensional projection. \n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"images/tSNE_scikit_learn.png\" width=\"650\" align=\"middle\">\n",
    "<div align=\"right\"> <font size=\"-3\">(credit: scikit-learn) </font></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4c72f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "I refer those interested to the original paper by [van der Maaten & Hinton](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4023d1-a2eb-4049-b730-a74a82820a36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "(and of course Bryan's talk that follows this one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13476e94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3) Clustering\n",
    "\n",
    "When I think about unsupervised learning, clustering is always at the forefront of my mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945982bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A cluster is a collection of objects that are all similar while also being dissimilar to other objects. \n",
    "\n",
    "Machine learning methods can be used to search for clusters within a multidimensional feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129bb68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "There are many algorithms that can be used for clustering. We will develop an intuition for how clustering works by examining a few of these methods in detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c8a2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A) K-means\n",
    "\n",
    "Every discussion of clustering starts with K-means, which is a simple method to identify clusters within a dataset.\n",
    "\n",
    "In brief, the user specifies the number of clusters to search for $K$, and the algorithm proceeds to divide sources in the feature space accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea6d37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The pseudocode is:  \n",
    "\n",
    "  1. choose the number of clusters, K\n",
    "  2. choose K random sources for initial cluster centers\n",
    "  3. assign each source to the nearest cluster (min distance between cluster center and source)\n",
    "  4. update the position of the cluster center to be the mean position of all in-cluster sources\n",
    "  5. repeat steps 3 and 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f388d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Once a user-specified stopping criteria is reached, such as $N$ cluster updates or some small fractional change in the cluster centers, further updates do not occur. \n",
    "\n",
    "We can visually demonstrate how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f312fd5-2580-4b33-b4ec-0e4870e801b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs([50,100,100], n_features=2, cluster_std=1, center_box=(-4,4), random_state=23)\n",
    "\n",
    "def kmeans_init(ax, fs=15):\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors='k')\n",
    "    ax.scatter(X[0:3,0], X[0:3,1], \n",
    "           marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e68cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans_init(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9ebe8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we assign every point to it's respective cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1f4dd-877e-479b-93f0-15300b04bb1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kmeans_assign(ax, X=X, fs=15):\n",
    "    clf = KMeans(n_clusters=3, init = X[0:3], max_iter=1, n_init=1)\n",
    "    clf.fit(X)\n",
    "    \n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    colors = cmap(clf.labels_/2)\n",
    "\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors=colors)\n",
    "    ax.scatter(X[0:3,0], X[0:3,1], \n",
    "           marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeee3b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans_assign(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406aedb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "And update the cluster centers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747217b7-8c1f-4e08-861c-480a64425a41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=3, init = X[0:3], max_iter=1, n_init=1)\n",
    "clf.fit(X)\n",
    "\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "colors = cmap(clf.labels_/2)\n",
    "\n",
    "center1 = clf.cluster_centers_\n",
    "\n",
    "def kmeans1(ax, X=X, fs=15):\n",
    "\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors=colors)\n",
    "    ax.scatter(X[0:3,0], X[0:3,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.6)\n",
    "    ax.scatter(center1[:,0], center1[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc119774",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans1(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96124b7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now repeat the assign and update steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1ab89-8531-401c-81a1-ed1673628c2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=3, init = X[0:3], max_iter=2, n_init=1)\n",
    "clf.fit(X)\n",
    "\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "colors = cmap(clf.labels_/2)\n",
    "center2 = clf.cluster_centers_\n",
    "\n",
    "def kmeans2(ax, fs=15):\n",
    "\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors=colors)\n",
    "    ax.scatter(X[0:3,0], X[0:3,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.5)\n",
    "    ax.scatter(center1[:,0], center1[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.75)\n",
    "    ax.scatter(center2[:,0], center2[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b205d23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans2(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da4440",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "And again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c176a-efae-4bc8-a0a1-7f67a9caa1e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=3, init = X[0:3], max_iter=3, n_init=1)\n",
    "clf.fit(X)\n",
    "\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "colors = cmap(clf.labels_/2)\n",
    "\n",
    "center3 = clf.cluster_centers_\n",
    "\n",
    "def kmeans3(ax, fs=15):\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors=colors)\n",
    "    ax.scatter(X[0:3,0], X[0:3,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.25)\n",
    "    ax.scatter(center1[:,0], center1[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.5)\n",
    "    ax.scatter(center2[:,0], center2[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.75)\n",
    "    ax.scatter(center3[:,0], center3[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ea55d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans3(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb7cef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "And fast forward to 100 iterations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ac164-6368-4665-a7e7-0d383c5e8fe7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=3, init = X[0:3], max_iter=100, n_init=1)\n",
    "clf.fit(X)\n",
    "\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "colors = cmap(clf.labels_/2)\n",
    "\n",
    "center4 = clf.cluster_centers_\n",
    "\n",
    "def kmeans100(ax, fs=15):\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors=colors)\n",
    "    ax.scatter(X[0:3,0], X[0:3,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.2)\n",
    "    ax.scatter(center1[:,0], center1[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.4)\n",
    "    ax.scatter(center2[:,0], center2[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.6)\n",
    "    ax.scatter(center3[:,0], center3[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7', alpha=0.8)\n",
    "    ax.scatter(center4[:,0], center4[:,1], \n",
    "               marker='X', s=500, c=[0,1,2], edgecolors='0.7')\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32f126",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "kmeans100(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bff87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "For this (extremely) simple example we can see that the clusters have converged. \n",
    "\n",
    "Under the hood, there are some user-defined parameters (e.g., distance metric – we used Euclidean), but one choice stands above all else..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930113b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "How in the world do we choose $K$? \n",
    "\n",
    "For the previous example, we created the data and therefore knew $K = 3$ was reasonable. When clustering new data, however, $K$ is truly unknown. By definition the algorithm will always find $K$ clusters, whether or not there are $K$ clusters present. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67129d69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Many clustering methods require the user to specify the number of clusters, prior to the application of the algorithm. There is also no natural metric for calculating if the specified number of clusters is correct. \n",
    "\n",
    "Other methods do not require an a priori number of clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77423b21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## B) DBSCAN\n",
    "\n",
    "The [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) algorithm searches for clusters using a user-specified radius $\\epsilon$.\n",
    "\n",
    "For each source the number of other sources within a distance $\\epsilon$ is counted. \n",
    "\n",
    "Sources with N >= `minPts` neighbors are considered core points (`minPts` is user-specified)  \n",
    "Sources with N < `minPts` neighbors, but at least one core neighbor, are \"reachable\" and part of the cluster  \n",
    "All sources that cannot be reached by a core point are considered \"outliers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389475c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"images/DBSCAN.png\" width=\"750\" align=\"middle\">\n",
    "<div align=\"right\"> <font size=\"-3\">(credit: wikiCommons) </font></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831e1d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can demonstrate this with the simulated data used in the KMeans example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753194bd-1490-4d63-bd2c-5bd2364469b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = DBSCAN(eps=1, min_samples=10)\n",
    "clf.fit(X)\n",
    "\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "colors = cmap((clf.labels_+1)/np.max(clf.labels_+1))\n",
    "\n",
    "def dbscan(ax, fs=15):\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors=colors)\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f54162",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "dbscan(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fbaa1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this case we can see that two clusters are identified, along with 5 outliers. \n",
    "\n",
    "Again - there are no metrics to know whether or not this is \"correct\" (or \"better\" than what we got from KMeans)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8f0c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "DBSCAN introduces the last component of unsupervised learning that we will discuss today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7de8b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4) Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411885c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A topic of great interest for the Rubin Observatory is anomaly detection. It is easy to understand why:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d152a-0002-40fe-a714-6f5ec068e97d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$~~~~$what if there are undiscovered galaxy subtypes?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef58e1f-8113-4c6a-8d6c-0b87d57644ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$~~~~$what if there is a Type III supernova?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a47e9-8c6a-47cb-8373-828d0c9c818b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$~~~~$what if we don't understand the systematics in the Rubin observing system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2768760",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "All of these questions might be answered via anomaly/outlier detection. \n",
    "\n",
    "How do we identify anomalies? Rather than discuss specific algorithms, I will review some of the types of outliers that we might be worthy of extra attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7005eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "(speaking of algorithms, we just saw one - `DBSCAN`  \n",
    "... or... you could use a KDE, and identify any sources/new observations with really low probability as outliers\n",
    "\n",
    "also, also – we have an entire lecture on autoencoders, which are a very promissing method for finding unusual behavior in the Rubin Observatory data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb690f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Global outliers** live far outside the entirety of the remaining data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d38b5-a0a9-493e-b6db-bdad1a6b4237",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "x = np.random.uniform(0,20,100)\n",
    "y = 3 + x**2 + np.random.normal(0,15, size=len(x))\n",
    "y[55] += 250\n",
    "\n",
    "def outlier(ax, fs=15):\n",
    "    ax.plot(x,y, 'o', mfc='None')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.annotate(\"outlier\",\n",
    "                xy=(x[55]+.25, y[55]), xycoords='data',\n",
    "                xytext=(7.5, 325), textcoords='data',\n",
    "                size=12, va=\"center\", ha=\"center\",\n",
    "                arrowprops=dict(arrowstyle=\"-|>\",\n",
    "                                connectionstyle=\"arc3,rad=-0.2\"))\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bee239",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "outlier(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5146c8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Contextual outliers** are observations that standout relative to their context:\n",
    "\n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"images/flu-season-coronavirus-pandemic.png\" width=\"750\" align=\"middle\">\n",
    "<div align=\"right\"> <font size=\"-3\">(credit: NY Times) </font></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6efa1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Local outliers** are unusual relative to a particular grouping of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc8da1-0c77-46a3-ba05-ee35ff55e0f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs([40,70], n_features=2, cluster_std=1, center_box=(-7,7), random_state=7)\n",
    "\n",
    "def local_outlier(ax, fs=15):\n",
    "    ax.scatter(X[:,0], X[:,1], facecolor=\"None\", edgecolors='k')\n",
    "    ax.annotate(\"local outlier\",\n",
    "                xy=(-2.85, 5.28), xycoords='data',\n",
    "                xytext=(-0.6, 5.9), textcoords='data',\n",
    "                size=12, va=\"center\", ha=\"center\",\n",
    "                arrowprops=dict(arrowstyle=\"-|>\",\n",
    "                                connectionstyle=\"arc3,rad=0.2\"))\n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a4ebc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "local_outlier(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c557c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Label outliers** are unusual relative to their their assigned class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58bd79-f22a-4ade-aff3-812fb753a252",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs([40,70], n_features=2, cluster_std=1, center_box=(-7,7), random_state=7)\n",
    "y[23] -= 1\n",
    "colors = cmap(y-0.3)\n",
    "\n",
    "def label_outlier(ax, fs=15):\n",
    "    ax.scatter(X[:,0], X[:,1], s=50, linewidths=2.5,\n",
    "               facecolor=\"None\", edgecolors=colors)\n",
    "    # ax.text(1.1, 4.2, 'label outlier', ha='center')\n",
    "    ax.annotate(\"label outlier\",\n",
    "                xy=(1.4, 4.1), xycoords='data',\n",
    "                xytext=(-1.5, 5.9), textcoords='data',\n",
    "                size=12, va=\"center\", ha=\"center\",\n",
    "                arrowprops=dict(arrowstyle=\"-|>\",\n",
    "                                connectionstyle=\"arc3,rad=-0.2\"))\n",
    "    \n",
    "    ax.set_xlabel('X1', fontsize=fs)\n",
    "    ax.set_ylabel('X2', fontsize=fs)\n",
    "    ax.tick_params(axis='both', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86b16b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5.5))\n",
    "label_outlier(ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81802704",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The detection/identification of anomalies can lead to exciting new astronomical discoveries (e.g., Quasars, Boyajian's star, etc). \n",
    "\n",
    "The *systematic* identification of anomalies is difficult, and in most astronomical data sets the population of outliers that are present is dominated by telescope artifacts/systematics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da42636",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "While I have just presented an entire lecture on the subject, I must confess – I am not a huge fan of unsupervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014400ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The main reason I feel this way is that there is no method to verify the accuracy of newly identified clusters.  \n",
    "$~~~~$Is $K$ actually 7, or should it have been 6?  \n",
    "$~~~~$If `minPts` doubles, does the number of clusters increase? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532763d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To answer these questions requires the collection of significant follow-up data. The follow-up is the hard part, and now that has been delayed by trying to identify clusters in the first place. \n",
    "\n",
    "Furthermore, I am not aware of any truly groundbreaking problems that were intractable and then miraculously solved by unsupervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f619f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Unsupervised learning is very useful in certain contexts:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1efa65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$~~~~$Exploratory analysis/visualization (density estimation, dimensionality reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb59f89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$~~~~$Semi-supervised learning ($N_\\mathrm{unlabeled} \\gg N_\\mathrm{labeled}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d501a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$~~~~$\"Live\" supervised models (e.g., find newly emerging classes)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "livereveal": {
   "height": 1024,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "width": 1024
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
