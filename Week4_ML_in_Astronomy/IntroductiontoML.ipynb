{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa672a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "#### Bryan Scott, CIERA/Northwestern\n",
    "Original version 0.1, August 2023\n",
    "\n",
    "Presented at LSSTC Data Science Fellowship Program Session 19: Machine Learning\n",
    "Adapted for the Astronomy Python Course at the University of Virginia, October 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e07a21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals for the Lecture\n",
    "\n",
    "Covering every ML technique, even in a popular library like scikit-learn would take much more than a week (or semester-long) session. That makes ML a very challenging subject to teach (or even introduce). \n",
    "\n",
    "As such, we'll focus on a bit of background with this notebook. The learning goal is to provide some background for the other notebooks, and to (start to) answer the following questions:\n",
    "\n",
    "- What is Machine Learning? How is different from statistics?\n",
    "- When do we use Machine Learning and what are its limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27752b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By the end of this talk, you should be able to \n",
    "\n",
    "- implement a classic Machine Learning algorithm and test it against real data. \n",
    "- articulate the limitations of a given Machine Learning model in terms of errors and uncertainties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753b5e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pedagogical Framework\n",
    "\n",
    "The pedagogical framework for this lecture is semi-active. I'll mostly be lecturing, but at a few points I'll ask you to discuss with your neighbor and share-back your thoughts. We'll then proceed to a hands-on tutorial implementing a ML algorithm on simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcddd62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Resources\n",
    "\n",
    "Machine Learning is an extremely active field. Some books that you might look at with increasing levels of sophistication:\n",
    "\n",
    "- $\\textit{Introduction to Machine Learning with Python}$, by Muller & Guido\n",
    "- $\\textit{Statistics, Data Mining, and Machine Learning in Astronomy}$, by IveziÄ‡, Connolly, VanderPlas, and Gray\n",
    "- $\\textit{Machine Learning for Physics and Astronomy}$, by Acquaviva $\\textbf{Tomorrow's Guest Lecturer!}$\n",
    "- $\\textit{Introduction to Statistical Learning}$, by James, Witten, Hastie & Tibshirani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be23661",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('aygSMgK3BEM', width=1200, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9378a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0fe565",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4575767",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('zMpEeag7kkM', width=1200, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf277973",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('cNxadbrN_aI', width=1200, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73525b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When do you want to use Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491f893",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When do we need machine learning? \n",
    "\n",
    "First, when a task is to complex to perform. That is, where it is\n",
    "- hard to specify an algorithm for solving the problem\n",
    "- the data is very large or complex\n",
    "\n",
    "Second, tasks that require adaptivity\n",
    "- where a task must change as a result of interaction with the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297cf01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd3155",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster as cluster\n",
    "import sklearn.ensemble as ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6534456",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dir(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e69fc",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dir(ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49287",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The (Formal) Structure of the Learning Problem\n",
    "\n",
    "A Learning Problem consists of the following parts\n",
    "\n",
    "$\\mathit{X}$ - a domain set or instance space of examples. These are usually n-dimensional vectors. We call the components of these vectors, $\\textit{features}$.\n",
    "\n",
    "$\\mathit{Y}$ - the label set, in supervised learning problems these are the set of possible $\\textit{labels}$ for each element in $\\mathit{X}$.\n",
    "\n",
    "$\\mathit{S}$ - the training set. These are ordered pairs of elements from $\\mathit{X}$ and $\\mathit{Y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e588b50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For many problems, we additionally assume there is some true mapping $f: \\mathit{X} \\rightarrow \\mathit{Y}$. These problems are called $\\textit{supervised learning}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf28ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formally, the learning problem is to learn, estimate, or approximate the true map $\\mathit{f}$ from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16517144",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We therefore distinguish between f and the output of the learner h, which is (at best) an approximation of f. \n",
    "\n",
    "$\\mathit{h}$ - the output of some learning algorithm. This is a prediction rule that tells us Y given some X. $\\mathit{h}: \\mathit{X} \\rightarrow  \\mathit{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791fd96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To review, the formal structure of a (supervised) learning problem consists of sets, X, Y, and S. Our goal is to learn a function from X $\\rightarrow$ Y from observed instances of ordered pairs X, Y. These observed instances are S - the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d96e14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "To enable learning, we will need a function, called a $\\textit{loss function}$, that tells us how wrong we are. Big values of the loss mean we're very wrong while small values mean we're less wrong. \n",
    "\n",
    "A common loss function is the mean squared error: \n",
    "\n",
    "$$\n",
    "L_S(h) = \\frac{1}{m} \\Sigma |h(x) - y(x)|^2\n",
    "$$\n",
    "\n",
    "for an estimator h, and features x for m training points $\\in$ S. Other loss functions exist, such as the $\\textit{absolute error}$ (rather than mean squared error), $\\textit{0-1 loss}$ (0 if the prediction of h is false, 1 if true; for binary classification problems), etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22af3a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For every distribution you sample data from, is it possible to construct an algorithm such that for any dataset of size m, you can gurantee with high confidence that the loss will be small? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc788b31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## No Free Lunch Theorem\n",
    "\n",
    "No, this is not possible due to a result called the \"No Free Lunch Theorem\" which (intuitively) states that a learning algorithm observing a subset of the instance spaces can learn a function that fits the subset but will fail on the unobserved half. \n",
    "\n",
    "That means there is no universal learning function that can be applied to all problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17bff53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is Learning difficult?\n",
    "\n",
    "A few reasons:\n",
    "- in general, the true function $\\mathit{f}$ could be very complicated and non-linear. \n",
    "- more generally, we don't know what model generated the dataset X. We assume it is sampled from some distribution $\\mathit{X} \\sim \\mathscr{D}$, where we do not have access to $\\mathscr{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa30ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taken together, this means we have to think carefully about our uncertainties on the outputs of learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70b261",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uncertainty in Machine Learning\n",
    "\n",
    "We can decompose the error on our estimate of the function h into two parts:\n",
    "\n",
    "- $\\epsilon_{app}$: the approximation error that arises from our learner not being 'perfect', or in other words, from the fact that $\\mathit{h} \\ne \\mathit{f}$ in general\n",
    "- $\\epsilon_{est}$: the estimation error that arises from $\\mathit{X} \\sim \\mathscr{D}$.\n",
    "\n",
    "Our estimator for our learning error is a combination of both terms. Importantly, it is estimated from the training data $S \\sim \\mathit{X} \\sim \\mathscr{D}$, which means that the error is itself a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066be4c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What can we do about our learning errors?\n",
    "\n",
    "As an example, we generally want a rich set of possible functions $\\mathit{h}$ to learn from. This decreases the $\\epsilon_{app}$ since we can pick better (less biased) $\\textit{h}$ than we could with a smaller set of functions to pick from, however $\\epsilon_{est}$ increases (the variance increases) with how complicated our set of possible $\\mathit{h}$ is.\n",
    "\n",
    "This is called the $\\textit{bias-variance tradeoff}$. In statistics, there is a formal limit on the bias and variance called the $\\textit{Cramer-Rao } bound$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecebdd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting it all together - Probably Approximately Correct (PAC) Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecd540",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What we've seen so far is that machine (or statistical) learning methods:\n",
    "\n",
    "- There is error due to failures of the learner, parameterized as $\\epsilon$. \n",
    "- There is error due to sampling from some unknown distribution, $S \\sim \\mathit{X} \\sim \\mathscr{D}$, parameterized as $\\delta$. \n",
    "\n",
    "This leads to a definition of the learning problem in terms of $\\textit{probably}$ ($\\delta$) $\\textit{approximately}$ ($\\epsilon$) $\\textit{correct}$ (PAC) learning. Probably captures the sampling of the data and approximately captures errors in our learning. This notion of PAC learnability allows you to make quantitative statements about, for example, how large a training set you need given how many possible functions you want to learn from the data. \n",
    "\n",
    "PAC learnability is a very common, but not the only, framework for discussing statistical/machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da9572",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Worry about the Data! \n",
    "\n",
    "Sophisticated techniques for reducing errors in the learner ($\\epsilon$) exist. We'll see many of them this week. Dealing with errors in the training sample - also called non-representativeness - are much more subtle. If our training data is biased, so will our inferences from that data! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2301e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics we will cover (but not all) in this session:\n",
    "\n",
    "- Unsupervised Machine Learning and Dimensionality Reduction\n",
    "- Supervised and Ensemble Learning\n",
    "- Convolutional Neural Networks\n",
    "- Graph Neural Networks\n",
    "- LLMs and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d2854",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
