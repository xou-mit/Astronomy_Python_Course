{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJokTsVMAdMf"
      },
      "source": [
        "# Graph Neural Networks\n",
        "\n",
        "### Author: [John F. Wu](https://jwuphysics.github.io/)\n",
        "\n",
        "Presented as a tutorial during [Session 19](https://github.com/LSSTC-DSFP/Session-19) of the [LSSTC Data Science Fellowship Program](https://www.lsstcorporation.org/lincc/fellowship_program).\n",
        "\n",
        "[![Open this notebook in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jwuphysics/LSSTC-DSFP-Session-19/blob/main/day4/Graph%20Neural%20Networks.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddjlARidIYHD"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, we will:\n",
        "0. learn about the basics of mathematic graphs and their ability to represent (astro)physical systems,\n",
        "1. create a cosmic graph from a catalog of dark matter halo properties,\n",
        "2. link together neighboring halos on the graph, and\n",
        "3. optimize a neural network that takes the cosmic graph as input, and outputs the stellar mass for every dark matter halo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0t1rUdqLqWR"
      },
      "source": [
        "<center><img src=\"https://ar5iv.labs.arxiv.org/html/2306.12327/assets/figures/cosmic-graph.png\" width=\"500\" height=\"500\"></center>\n",
        "\n",
        "**Figure**: A graph of dark matter halos, colored by their halo masses, in a 50 Mpc volume connected using a linking length of 5 Mpc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I_BKtSELvOj"
      },
      "source": [
        "\n",
        "\n",
        "**References**\n",
        "- [Wu & Jespersen (2023)](https://ui.adsabs.harvard.edu/abs/2023arXiv230612327W/abstract) ICML ML4Astro workshop paper; see also the [halo-gnns](https://github.com/jwuphysics/halo-gnns) repository\n",
        "- [Nelson et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019ComAC...6....2N/abstract) for the [Illustris TNG300](https://www.tng-project.org/data/) hydrodynamic simulation\n",
        "\n",
        "**Other Resources**\n",
        "- [KITP GNN tutorial notebook](https://github.com/DataDrivenGalaxyEvolution/galevo23-tutorials/blob/main/week-3/KITP-CCA-GNN-tutorial.ipynb) and accompanying [recording](https://www.dropbox.com/scl/fo/tgpwe1ljr4i9go4ajnsq6/h?dl=0&preview=2023-02-03++CCA+KITP+-+Tutorial+-+Christian+Kragh+Jespersen+-+Alpha+Beta.mp4&rlkey=mwl1zi14okj5k1pnbflcgnz3l) by Christian Kragh Jespersen\n",
        "- [Pytorch-Geometric documentation](https://pytorch-geometric.readthedocs.io/en/latest/) and  [tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)\n",
        "- [Stanford CS224W tutorials](https://medium.com/stanford-cs224w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzGjlwUcP3xH"
      },
      "source": [
        "## Setting up the notebook\n",
        "\n",
        "1. (*Optional*) If you want to use a GPU runtime, make sure to set it before getting started. At the top of this page, click on `Runtime` > `Change runtime type` > Under `Hardware accelerate`, select `T4 GPU`. However, note that Colab only allocates a limited amount of GPU time, and that most of this notebook should run in a reasonable amount of time even if you use a CPU instead.\n",
        "2. Run the **Install Pytorch-Geometric and other necessary packages** cell below in order to install the required Python packages. You can click the triangle \"play button\" in order to run each cell. If you see a green checkmark to the left of the cell, then it should have successfully installed the packages! (My cell also says 45s below the checkmark, which is the duration of time it took to install the packages.)\n",
        "3. Run the **Import Python packages** cell in order to be able to use packages like `pytorch`, `pytorch_geometric`, `numpy`, `pandas`, and `matplotlib`.\n",
        "4. Run the **Download TNG300 simulation data** cell in order to get the subhalo catalogs that have been put together for this tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI_YTddiG21V",
        "outputId": "db1a3320-3806-41ed-f4c9-96c7eb02d81e"
      },
      "outputs": [],
      "source": [
        "#@title Install Pytorch-Geometric and other necessary packages\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -qf https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -qf https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -qf https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -qf https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HR9fTq-GYhrp"
      },
      "outputs": [],
      "source": [
        "# @title Import Python packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import scipy\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader, RandomNodeLoader\n",
        "from torch_geometric.utils import to_networkx, to_undirected\n",
        "from torch_geometric.nn import (\n",
        "    MessagePassing, GCNConv, SAGEConv, EdgeConv,\n",
        ")\n",
        "from torch_cluster import radius_graph\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XbLRUqA3VgOK"
      },
      "outputs": [],
      "source": [
        "# @title Download TNG300 simulation data\n",
        "!wget -q -O subhalos.parquet https://www.dropbox.com/scl/fi/7vng9fx9q6gamu4abomhn/subhalos-hydro.parquet?rlkey=r340qt3ju430qlzkr1t5c7gmy&dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E079RdIKZmh4"
      },
      "source": [
        "## Exploratory data analysis\n",
        "\n",
        "It's always nice to get some intuition for what's in the data set. Let's get started by checking the number of rows and columns, and sampling some of the rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "jH-hPTqkZvgJ",
        "outputId": "fa078292-bbcf-4cae-cd5d-85be7ed62dea"
      },
      "outputs": [],
      "source": [
        "subhalos = pd.read_parquet(\"subhalos.parquet\")\n",
        "print(subhalos.shape)\n",
        "subhalos.sample(5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtHEoA__c6bR"
      },
      "source": [
        "Since we know that these are all subhalo properties, I'm going to make the column names more concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "B6Hk2TxEc5kV",
        "outputId": "cc0e4c0b-24a6-4a74-ced3-1aeb8ae570aa"
      },
      "outputs": [],
      "source": [
        "subhalo_colname_mapping = {\n",
        "    c: c[8:]\n",
        "    for c in subhalos.columns\n",
        "    if (c.startswith(\"subhalo_\") and c != \"subhalo_id\")\n",
        "}\n",
        "\n",
        "subhalos.rename(subhalo_colname_mapping, axis=1, inplace=True)\n",
        "\n",
        "subhalos.sample(5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "P_tbQiXC7QAO",
        "outputId": "a0e1095d-e4c2-4f1b-995f-2bb736519340"
      },
      "outputs": [],
      "source": [
        "# another way to look over the data\n",
        "subhalos.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L8dinIJ9U0n"
      },
      "outputs": [],
      "source": [
        "# some of these positions are negative -- let's make sure they're above 0\n",
        "subhalos[[\"x\", \"y\", \"z\"]] -= subhalos[[\"x\", \"y\", \"z\"]].min(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtEzGOGvahdg"
      },
      "source": [
        "Let's get a sense of how the data are distributed from a scientific perspective. But we have over half a million halos! Let's get a smaller sample just for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkO9h4tUbHLb",
        "outputId": "a743e0f7-79cf-4d6d-8284-9a2d8a1e18c9"
      },
      "outputs": [],
      "source": [
        "# get a smaller subset of rows\n",
        "df = subhalos[(subhalos.x < 50) & (subhalos.y < 50) & (subhalos.z < 50)].copy()\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "l4j1wJxpZ42i",
        "outputId": "3c4e42be-3396-452b-f024-9319ac360b6b"
      },
      "outputs": [],
      "source": [
        "# @title Let's plot the **Stellar mass-Halo mass relation**:\n",
        "plt.figure(figsize=(3, 3), dpi=150)\n",
        "plt.scatter(\n",
        "    x=df[\"loghalomass\"],\n",
        "    y=df[\"logstellarmass\"],\n",
        "    s=5,\n",
        "    c=np.where(df[\"is_central\"], \"C3\", \"C0\"),\n",
        "    edgecolors=\"none\",\n",
        ")\n",
        "\n",
        "plt.text(0.95, 0.2, \"Central\", color=\"C3\", transform=plt.gca().transAxes, ha=\"right\", fontsize=16)\n",
        "plt.text(0.95, 0.1, \"Satellite\", color=\"C0\", transform=plt.gca().transAxes, ha=\"right\", fontsize=16)\n",
        "\n",
        "plt.xlabel(r\"$\\log(M_{\\rm halo}/M_\\odot)$\")\n",
        "plt.ylabel(r\"$\\log(M_\\bigstar/M_\\odot)$\")\n",
        "plt.grid(alpha=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "WJji1p4ecdBm",
        "outputId": "2a1fd622-c863-454e-9a6d-10b9d6a95a62"
      },
      "outputs": [],
      "source": [
        "# @title Let's also take a look at the **3d distribution of galaxies/subhalos**:\n",
        "\n",
        "fig = plt.figure(figsize=(7, 8), dpi=150)\n",
        "ax = fig.add_subplot(projection=\"3d\")\n",
        "\n",
        "fontsize = 12\n",
        "pos = df[[\"x\", \"y\", \"z\"]] - 25\n",
        "\n",
        "# plot nodes\n",
        "sc = ax.scatter(\n",
        "    pos.x,\n",
        "    pos.y,\n",
        "    pos.z,\n",
        "    s=(df.loghalomass-9)**3,\n",
        "    vmin=9,\n",
        "    vmax=12,\n",
        "    alpha=0.8,\n",
        "    edgecolor='k',\n",
        "    c=df.logstellarmass,\n",
        "    cmap=\"plasma\",\n",
        "    linewidths=0.1\n",
        ")\n",
        "fig.subplots_adjust(left=0.1, right=0.9)\n",
        "\n",
        "cb = fig.colorbar(sc, aspect=50, pad=-0.02, location='top')\n",
        "cb.set_label(r\"log($M_\\bigstar/M_\\odot$)\", fontsize=12)\n",
        "\n",
        "ax.set_xlim(-25, 25)\n",
        "ax.set_ylim(-25, 25)\n",
        "ax.set_zlim(-25, 25)\n",
        "ax.set_xlabel(\"X [Mpc]\", fontsize=12)\n",
        "ax.set_ylabel(\"Y [Mpc]\", fontsize=12)\n",
        "ax.set_zlabel(\"Z [Mpc]\", fontsize=12)\n",
        "ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
        "ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
        "ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
        "ax.xaxis._axinfo[\"grid\"]['color'] =  (0.5,0.5,0.5,0.2)\n",
        "ax.yaxis._axinfo[\"grid\"]['color'] =  (0.5,0.5,0.5,0.2)\n",
        "ax.zaxis._axinfo[\"grid\"]['color'] =  (0.5,0.5,0.5,0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RYzPI5iEA31"
      },
      "source": [
        "## Mathematical graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar3nNcieklcD"
      },
      "source": [
        "This last plot shows the positions of subhalos in the TNG300 catalog. These data sets are sometimes called \"point clouds\" in computer vision, and they will serve as the launching point for our foray into graphs and graph neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfJzmXkunpi9"
      },
      "source": [
        "Graphs can be represented in many ways, but here we will focus on implementing them in PyG ([`torch_geometric.data.Data`](https://pytorch-geometric.readthedocs.io/en/stable/generated/torch_geometric.data.Data.html#torch_geometric.data.Data)). This `Data` class is a flexible way to store graphs and general features (aka *attributes*). Let's look at the following graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACb0ZuDLn0cz",
        "outputId": "9a0916be-d089-4ca8-cd8d-c8923cc7457d"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[2.5], [4.5], [1.7], [1.0]], dtype=torch.float)\n",
        "\n",
        "edge_index = torch.tensor([[0, 1, 1, 2, 1, 3],\n",
        "                           [1, 0, 2, 1, 3, 1]], dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTyNKm9asHwc"
      },
      "source": [
        "First, let's look at how the graph is connected via the `data.edge_index` attribute. We'll check out the other attribute, `data.x` (which holds the node features) in a moment.\n",
        "\n",
        "The `edge_index` is a *sparse* representation of a graph's adjacency matrix, $\\mathbf{A}$. The connectivity of the above graph can be described by its adjacency matrix:\n",
        "\n",
        "$$ \\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "1 & 0 & 1 & 1 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "or equivalently, the `edge_index` representation that we wrote above:\n",
        "\n",
        "```python\n",
        "edge_index = torch.tensor([[0, 1, 1, 2, 1, 3],\n",
        "                           [1, 0, 2, 1, 3, 1]], dtype=torch.long)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2RhlcJ6uppu"
      },
      "source": [
        "We can make note of several properties already:\n",
        "1. The elements of the adjacency matrix $\\mathbf{A}_{ij}$ between nodes $i$ and $j$ are 1 if an edge exists, and 0 if an edge doesn't exist between the two nodes.\n",
        "2. The number of non-zero elements in $\\mathbf{A}$ is equal to the length of a row in `edge_index`.\n",
        "3. The above graph is an *undirected* (aka *bidirected*) graph. As a result, its adjacency matrix $\\mathbf{A}$ is symmetric.\n",
        "4. If the graph has $\\mathcal{V}$ nodes and $\\mathcal{E}$ *directed* edges, then the graph is sparse when $\\mathcal{E} \\ll \\mathcal{V}^2$. In such cases, the `edge_index` representation (requiring $2 \\times \\mathcal{E}$ elements) is more efficient than writing out the entire adjacency matrix.\n",
        "5. None of the nodes connect to themselves, and therefore the diagonal of $\\mathbf{A}$ is all zeros. If the graph contains *self-loops*, then those diagonal elements would contain ones.\n",
        "6. Everything is 0-indexed here because we're using Python. Note that in mathematics we conventionally see 1-indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wykYFLwUrjry"
      },
      "source": [
        "We can use another package, `networkx`, to make basic visualizations of these graphs. In this case, we are drawing the $\\mathcal{V} = 4$ nodes and the $\\mathcal{E} = 6$ directed edges. We are also scaling each node's size by its (single) feature stored in `data.x`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "mZKhiu26o3FZ",
        "outputId": "4968177b-9b4a-464e-ac10-7165bc93ef4b"
      },
      "outputs": [],
      "source": [
        "nxGraph = to_networkx(data)\n",
        "nx.draw(nxGraph, node_size=(10*data.x)**2, node_color=\"#ff6361\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iskzq2EbsAM9"
      },
      "source": [
        "### **Exercise 1**\n",
        "**What is the index of the node that connects to the other three nodes?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Ze6JWsUV8T"
      },
      "source": [
        "<details>\n",
        "<summary><b>Answer</b></summary>\n",
        "  \n",
        "The node indexed to `1` is connected to the other nodes. You can verify corresponding row and column in the adjacency matrix $\\mathbf{A}_{i1}$ and $\\mathbf{A}_{1j}^T$ are $(1, 0, 1, 1)$, signifying that the node has undirected edges connecting to the other three nodes.\n",
        "\n",
        "One way to visualize this is to run the following code\n",
        "```python\n",
        "nxGraph = to_networkx(data)\n",
        "nx.draw(nxGraph, node_size=(10*data.x)**2, node_color=\"#ff6361\", with_labels=True)\n",
        "```\n",
        "  \n",
        "We can also verify that the node feature at the index `1` has the largest value, in accordance with the graph plotted above. By running\n",
        "```python\n",
        "for V_i, x_i in enumerate(data.x):\n",
        "    print(f\"Node {V_i}: x = {x_i.item():g}\")\n",
        "```\n",
        "or by looking back to the constructor for `data`, we can see that the node feature at `data.x[1]` has the largest value.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ6EMoRC2FcZ"
      },
      "source": [
        "### **Exercise 2**\n",
        "\n",
        "**Create another `torch_geometric.data.Data` object with the same nodes and edge features as above, but also with self-loops.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzkSK8yUXMr7"
      },
      "source": [
        "<details>\n",
        "<summary><b>Answer</b></summary>\n",
        "  \n",
        "We can construct a new versions of `edge_index` with the self-loops:\n",
        "```python\n",
        "edge_index = torch.tensor([[0, 1, 1, 2, 1, 3, 0, 1, 2, 3],\n",
        "                           [1, 0, 2, 1, 3, 1, 0, 1, 2, 3]], dtype=torch.long)\n",
        "```\n",
        "Since the node features remain the same, we don't need to update that. However, we do need to remake the PyG `Data` object, and we can optionally plot it again:\n",
        "```python\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "nxGraph = to_networkx(data)\n",
        "nx.draw(nxGraph, node_size=(10*data.x)**2)\n",
        "```\n",
        "\n",
        "Another solution is to use [`torch_geometric.utils.add_self_loops`](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.add_self_loops), which takes in the `edge_index` and an optional `edge_weight`, and returns the output `edge_index` and `edge_weight`.\n",
        "\n",
        "If we ran it here, we'd find:\n",
        "```python\n",
        "from torch_geomegtric.utils import add_self_loops\n",
        "edge_index, _ = add_self_loops(edge_index)\n",
        "print(edge_index)\n",
        "\n",
        "> (tensor([[0, 1, 1, 2, 1, 3, 0, 1, 2, 3],\n",
        ">         [1, 0, 2, 1, 3, 1, 0, 1, 2, 3]]),\n",
        "> None)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubLazev6TUBZ"
      },
      "source": [
        "### **Exercise 3**\n",
        "\n",
        "**Fill in the `data.edge_index` attribute to connect all nodes (minus self-loops) the graph shown below.**\n",
        "\n",
        "Note that a graph without self-loops is called a *simple graph*, and a fully connected simple undirected graph is called a *complete graph*.\n",
        "\n",
        "Hint: You may recall that a complete graph with $\\mathcal{V}$ nodes will have $\\mathcal{V}(\\mathcal{V}-1)/2$ undirected edges, but using PyG we need to specify both directions for the edges!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "pg_Nsxj7Tnay",
        "outputId": "4cdef882-7fa8-45bb-f46a-eb646c41d8ee"
      },
      "outputs": [],
      "source": [
        "# add indices below\n",
        "edge_index = torch.tensor([[], []], dtype=torch.long)\n",
        "\n",
        "# just stick random entries below\n",
        "x_features = torch.randn((4, 1))\n",
        "\n",
        "data = Data(edge_index=edge_index, x=x_features)\n",
        "\n",
        "nxGraph = to_networkx(data)\n",
        "nx.draw(nxGraph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3SpYeL5UROL"
      },
      "source": [
        "<details>\n",
        "<summary><b>Answer</b></summary>\n",
        "  \n",
        "One solution is to write out all of the indices, e.g.,\n",
        "\n",
        "```python\n",
        "edge_index = torch.tensor([[1, 2, 3, 2, 3, 3, 0, 1, 2, 0, 1, 0],\n",
        "                           [0, 1, 2, 0, 1, 0, 1, 2, 3, 2, 3, 3]], dtype=torch.long)\n",
        "```\n",
        "\n",
        "Another solution is to write out half of these edges, and then use the helper function [`torch.utils.to_undirected`](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.to_undirected) like such:\n",
        "```python\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "edge_index = torch.tensor([[1, 2, 3, 2, 3, 3],\n",
        "                           [0, 1, 2, 0, 1, 0]], dtype=torch.long)\n",
        "edge_index = to_undirected(edge_index)\n",
        "```\n",
        "\n",
        "Finally we can construct our `Data` object, and plot it:\n",
        "```python\n",
        "x_features = torch.randn((4,1))\n",
        "\n",
        "data = Data(edge_index=edge_index, x=x_features)\n",
        "\n",
        "nxGraph = to_networkx(data)\n",
        "nx.draw(nxGraph)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolZumK8kr1"
      },
      "source": [
        "### Some final technical notes\n",
        "* We are writing out the edge indices by specifying all of the $i$ indices, and then all of the $j$ indices. It is also possible to write out a tensor of $(i, j)$ indices, and then transpose that tensor. However, we must ensure that this representation is *contiguous* in memory, e.g.:\n",
        "\n",
        "    ```python\n",
        "  x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
        "\n",
        "  edge_index = torch.tensor([[0, 1],\n",
        "                             [1, 0],\n",
        "                             [1, 2],\n",
        "                             [2, 1]], dtype=torch.long)\n",
        "\n",
        "  data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
        "    ```\n",
        "* For indices and other integer-typed attributes, make sure you specify the data type as `dtype=torch.long`.\n",
        "* This tutorial only focuses on homogeneous graphs, i.e., all of the nodes correspond to galaxies/subhalos, and all edges encode relationships between them. However, it's possible to build graphs with heterogeneous nodes or edges. For example, the Atacama Large Millimeter/submillimeter Array (ALMA) comprises both 7-m and 12-m antennas (see below); they could be represented on [*heterogeneous graphs*](https://pytorch-geometric.readthedocs.io/en/latest/notes/heterogeneous.html?highlight=heterogeneous).\n",
        "<center><img src=\"https://www.almaobservatory.org/wp-content/uploads/2023/05/file-1-copy-2048x988.jpeg\" width=\"600\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT9S47mvD_Gr"
      },
      "source": [
        "## Building a cosmic graph using PyG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AvfusrVd3QE"
      },
      "outputs": [],
      "source": [
        "# select a smaller subset of subhalos that are better resolved by the simulation\n",
        "cut = subhalos[\"loghalomass\"] > 11\n",
        "\n",
        "df = subhalos[cut].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrssFYOsjqRo"
      },
      "outputs": [],
      "source": [
        "# get input node features, `x`, and targets, `y`\n",
        "x = torch.tensor(df[[\"loghalomass\", \"logvmax\"]].values, dtype=torch.float)\n",
        "y = torch.tensor(df[[\"logstellarmass\"]].values, dtype=torch.float)\n",
        "\n",
        "# also keep track of positions\n",
        "pos = torch.tensor(df[[\"x\", \"y\", \"z\"]].values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjZ7G3hKk9Zc"
      },
      "outputs": [],
      "source": [
        "D_link = 5\n",
        "\n",
        "# get all edges\n",
        "kd_tree = scipy.spatial.KDTree(pos, leafsize=25, boxsize=205.0001/0.6774)\n",
        "edge_index = kd_tree.query_pairs(r=D_link, output_type=\"ndarray\")\n",
        "\n",
        "# use tensor, add reverse pairs\n",
        "edge_index = torch.Tensor(edge_index).t().contiguous().type(torch.long)\n",
        "edge_index = to_undirected(edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eZ_6Pv7mytL"
      },
      "outputs": [],
      "source": [
        "# compute distances and velocity differences between pairwise node combinations\n",
        "row, col = edge_index\n",
        "dist = torch.tensor(np.linalg.norm(pos[row] - pos[col], axis=1)).reshape(-1, 1)\n",
        "\n",
        "vel = torch.tensor(df[[\"vx\", \"vy\", \"vz\"]].values, dtype=torch.float)\n",
        "veldiff = torch.tensor(np.linalg.norm(vel[row] - vel[col], axis=1)).reshape(-1, 1)\n",
        "\n",
        "edge_attr = torch.cat([dist, veldiff], dim=-1).type(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbeXY5Nck0jh"
      },
      "outputs": [],
      "source": [
        "data = Data(\n",
        "    x=x,\n",
        "    y=y,\n",
        "    pos=pos,\n",
        "    edge_index=edge_index,\n",
        "    edge_attr=edge_attr,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDE-QPMbFdZQ",
        "outputId": "b6945963-75cd-4943-86df-5814dc93a079"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32dVgFlEJLi"
      },
      "source": [
        "## Setting up the problem: Predict $M_\\star$ from $M_{\\rm halo}$ with GNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJnshZ6tzieA"
      },
      "outputs": [],
      "source": [
        "# a smaller subset of the full data\n",
        "train_mask = (df[\"z\"] <= 200) & (df[\"x\"] < 50) & (df[\"y\"] < 50)\n",
        "valid_mask = (df[\"z\"] > 210) & (df[\"x\"] < 50) & (df[\"y\"] < 50)\n",
        "\n",
        "# note -- this is the equivalent of `np.argwhere()`\n",
        "train_indices = torch.tensor(train_mask.values).nonzero(as_tuple=True)[0]\n",
        "valid_indices = torch.tensor(valid_mask.values).nonzero(as_tuple=True)[0]\n",
        "\n",
        "assert (set(train_indices) & set(valid_indices)) == set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeaAs7jueibj"
      },
      "source": [
        "## Train a baseline model\n",
        "\n",
        "In this case, we will train a [`scikit-learn` Random Forest (RF) regression model](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). Refer back to Viviana's lectures for details on decision tree-based algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOvXqUqxt4Kb"
      },
      "outputs": [],
      "source": [
        "# train-test split\n",
        "df_train = df[train_mask].copy()\n",
        "df_valid = df[valid_mask].copy()\n",
        "\n",
        "X_train = df_train[[\"loghalomass\", \"logvmax\"]].values\n",
        "y_train = df_train[[\"logstellarmass\"]].values\n",
        "X_valid = df_valid[[\"loghalomass\", \"logvmax\"]].values\n",
        "y_valid = df_valid[[\"logstellarmass\"]].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVlIyrWTiYH7"
      },
      "outputs": [],
      "source": [
        "# random forest <3\n",
        "rf = RandomForestRegressor(n_estimators=25, random_state=42)\n",
        "rf.fit(X_train, y_train.ravel())\n",
        "p_valid = rf.predict(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "m_syheBft98n",
        "outputId": "b51109e4-45c8-4664-bd86-090f498d516d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 4), dpi=150)\n",
        "plt.scatter(y_valid, p_valid, s=5, edgecolors=\"none\", c=df_valid.loghalomass.values, vmin=11, vmax=14)\n",
        "plt.colorbar(label=r\"$\\log(M_{\\rm halo}/M_\\odot)$\")\n",
        "plt.grid(alpha=0.15)\n",
        "plt.xlim(9, 12)\n",
        "plt.ylim(9, 12)\n",
        "plt.xlabel(r\"True $\\log(M_\\bigstar/M_\\odot)$\", fontsize=12)\n",
        "plt.ylabel(r\"Predicted $\\log(M_\\bigstar/M_\\odot)$\", fontsize=12)\n",
        "\n",
        "plt.title(f\"RMSE(RF) = {((p_valid.ravel() - y_valid.ravel())**2).mean()**0.5:.4f} dex\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXkxrdM3EGki"
      },
      "source": [
        "## Creating a simple graph neural network\n",
        "\n",
        "This network uses SAGEConv (or GraphSAGE), which uses node-only features and the graph connectivity. In other words, the subhalos know about their own and neighboring subhalos' masses, but nothing about positional or velocity differences between it and its neighbors. See the paper by [Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iitj3GyT1408"
      },
      "outputs": [],
      "source": [
        "class SAGEGraphConvNet(torch.nn.Module):\n",
        "    \"\"\"A simple GNN built using SAGEConv layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in=2, n_hidden=128, n_out=1):\n",
        "        super(SAGEGraphConvNet, self).__init__()\n",
        "        self.conv1 = SAGEConv(n_in, n_hidden)\n",
        "        self.conv2 = SAGEConv(n_hidden, n_hidden)\n",
        "        self.fc = nn.Linear(n_hidden, n_out, bias=True)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_in, n_hidden, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(n_hidden),\n",
        "            nn.Linear(n_hidden, n_out, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x0, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x1 = self.conv1(x0, edge_index)\n",
        "        x2 = self.conv2(F.relu(x1), edge_index)\n",
        "        convs_out = self.fc(F.relu(x2))\n",
        "        mlp_out = self.mlp(x0)\n",
        "        return convs_out + mlp_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDg0wAUu5quR"
      },
      "outputs": [],
      "source": [
        "model = SAGEGraphConvNet().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8zUcB52Mejt"
      },
      "source": [
        "### **Exercise 4**\n",
        "\n",
        "**What are the dimensions of the inputs and outputs for each layer in the initialized model (`conv1`, `conv2`, `fc`, and `mlp`)?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsDcjO1oMynM"
      },
      "source": [
        "<details>\n",
        "<summary><b>Answer</b></summary>\n",
        "\n",
        "All of the layers are two-dimensions, with the first dimension equal to the batch size, which I'll call `bs`.\n",
        "\n",
        "For `conv1`: The inputs are of shape `(bs, 2)` since there are two node features. The outputs are of shape `(bs, 128)`, since `n_hidden = 128` when we initialized the model.\n",
        "\n",
        "For `conv2`: The intputs are now shape `(bs, 128)` since it takes the outputs of the previous layer (after applying a rectified linear unit, or ReLU). The output is again `(bs, 128)`.\n",
        "\n",
        "For `fc`: The inputs have shape `(bs, 128)` since it receives the previous layer's output. The output has shape `(bs, 1)`.\n",
        "\n",
        "For `mlp`: The inputs have shape `(bs, 2)`, as it ingests the node-level features as inputs. There is a hidden layer, non-linear activation, and normalization layer, and finally the output is of shape `(bs, 1)`.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsaQKxOFOaIB"
      },
      "source": [
        "### **Exercise 5**\n",
        "\n",
        "**Modify the SAGEGraphConvNet in the following way:**\n",
        "\n",
        "\n",
        "1. **Remove the `fc` layer**.\n",
        "2. **Adjust the `mlp` layer such that it ingests both the node input features, as well as the activated outputs of `conv2`**.\n",
        "\n",
        "*Hint: for part two, you'll need to concatenate the features using `torch.cat()` along the last dimension.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZsY9rGZQIEB"
      },
      "source": [
        "<details>\n",
        "<summary><b>Answer</b></summary>\n",
        "\n",
        "One solution is like such:\n",
        "\n",
        "```python\n",
        "class SAGEGraphConvNet(torch.nn.Module):\n",
        "    \"\"\"A simple GNN built using SAGEConv layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in=2, n_hidden=16, n_out=1):\n",
        "        super(SAGEGraphConvNet, self).__init__()\n",
        "        self.conv1 = SAGEConv(n_in, n_hidden)\n",
        "        self.conv2 = SAGEConv(n_hidden, n_hidden)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_in + n_hidden, n_hidden, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(n_hidden),\n",
        "            nn.Linear(n_hidden, n_out, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x0, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x1 = self.conv1(x0, edge_index)\n",
        "        x2 = self.conv2(F.relu(x1), edge_index)\n",
        "        return self.mlp(torch.cat([F.relu(x2), x0], dim=-1))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E1uFLHu4zL1"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, optimizer, device=\"cuda\"):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss_total = 0\n",
        "    for data in dataloader:\n",
        "        data.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(data)\n",
        "        loss = F.mse_loss(y_pred, data.y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_total += loss.item()\n",
        "\n",
        "    return loss_total / len(dataloader)\n",
        "\n",
        "def validate(dataloader, model, device=\"cuda\"):\n",
        "    model.eval()\n",
        "\n",
        "    uncertainties = []\n",
        "    loss_total = 0\n",
        "\n",
        "    y_preds = []\n",
        "    y_trues = []\n",
        "\n",
        "    for data in dataloader:\n",
        "        with torch.no_grad():\n",
        "            data.to(device)\n",
        "            y_pred = model(data)\n",
        "            loss = F.mse_loss(y_pred, data.y)\n",
        "\n",
        "            loss_total += loss.item()\n",
        "            y_preds += list(y_pred.detach().cpu().numpy())\n",
        "            y_trues += list(data.y.detach().cpu().numpy())\n",
        "\n",
        "    y_preds = np.concatenate(y_preds)\n",
        "    y_trues = np.array(y_trues)\n",
        "\n",
        "    return (\n",
        "        loss_total / len(dataloader),\n",
        "        y_preds,\n",
        "        y_trues,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3gtxkdf4MC7"
      },
      "outputs": [],
      "source": [
        "bs = 256\n",
        "lr = 1e-2\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=lr,\n",
        ")\n",
        "\n",
        "train_loader = RandomNodeLoader(\n",
        "    data.subgraph(train_indices),\n",
        "    num_parts=(len(train_indices) // bs),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "valid_loader = RandomNodeLoader(\n",
        "    data.subgraph(valid_indices),\n",
        "    num_parts=(len(valid_indices) // bs),\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8j_AUWc4Yg2",
        "outputId": "402d0b45-b292-47c9-dd4c-48a5b85ae167"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "print(f\"Epoch    Train loss   Valid Loss      RSME \")\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    train_loss = train(train_loader, model, optimizer, device=device)\n",
        "    valid_loss, p, y  = validate(valid_loader, model, device=device)\n",
        "\n",
        "    print(f\" {epoch + 1: >4d}    {train_loss: >9.5f}    {valid_loss: >9.5f}    {np.sqrt(np.mean((p - y.flatten())**2)): >10.6f}\")\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "5z0OPFNk6HYx",
        "outputId": "ad8c2387-bdd5-4485-af72-73b2c1ff54fe"
      },
      "outputs": [],
      "source": [
        "valid_loss, p, y  = validate(valid_loader, model, device=device)\n",
        "\n",
        "plt.figure(figsize=(5, 4), dpi=150)\n",
        "plt.scatter(y, p, s=5, edgecolors=\"none\", c=df.iloc[valid_indices].loghalomass.values, vmin=11, vmax=14)\n",
        "plt.colorbar(label=r\"$\\log(M_{\\rm halo}/M_\\odot)$\")\n",
        "plt.grid(alpha=0.15)\n",
        "plt.xlim(9, 12)\n",
        "plt.ylim(9, 12)\n",
        "plt.xlabel(r\"True $\\log(M_\\bigstar/M_\\odot)$\", fontsize=12)\n",
        "plt.ylabel(r\"GNN Predicted $\\log(M_\\bigstar/M_\\odot)$\", fontsize=12)\n",
        "\n",
        "plt.title(f\"RMSE(Simple GNN) = {((p.ravel() - y.ravel())**2).mean()**0.5:.4f} dex\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFleWOdIBrd1"
      },
      "source": [
        "There are many things to vary here in order to help build intuition, for example:\n",
        "- `SAGEConv` allows you to change the aggregation function from the default of `mean` pooling to anything else.\n",
        "- What happenes when you change the number of hidden layers in the network?\n",
        "- What happens when you increase or decrease the batch size?\n",
        "- How do these hyperparameters (qualitatively) covary with each other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p7Knjbp8kxz"
      },
      "source": [
        "## Bonus! A far more complicated GNN with edge features and node-edge interactions\n",
        "\n",
        "Check out the GNN implemented below, which is based on the one from the [Wu & Jespersen (2023)](https://arxiv.org/abs/2306.12327) paper. Can you sketch out the architecture based on the code?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ9fD7l4y7Rb"
      },
      "outputs": [],
      "source": [
        "class EdgeInteractionLayer(MessagePassing):\n",
        "    \"\"\"Interaction network layer with node + edge layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_hidden, n_latent, aggr='sum'):\n",
        "        super(EdgeInteractionLayer, self).__init__(aggr)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_in, n_hidden, bias=True),\n",
        "            nn.LayerNorm(n_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_hidden, bias=True),\n",
        "            nn.LayerNorm(n_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_latent, bias=True),\n",
        "        )\n",
        "\n",
        "        self.messages = 0.\n",
        "        self.input = 0.\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "\n",
        "    def message(self, x_i, x_j, edge_attr):\n",
        "\n",
        "        self.input = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
        "        self.messages = self.mlp(self.input)\n",
        "\n",
        "        return self.messages\n",
        "\n",
        "class EdgeInteractionGNN(nn.Module):\n",
        "    \"\"\"Graph net over nodes and edges with multiple unshared layers, and sequential layers with residual connections.\n",
        "    Self-loops also get their own MLP (i.e. galaxy-halo connection).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_layers, D_link, node_features=2, edge_features=2, hidden_channels=16, aggr=\"sum\", latent_channels=16, n_out=1, n_unshared_layers=8, loop=True, use_global_pooling=True):\n",
        "        super(EdgeInteractionGNN, self).__init__()\n",
        "\n",
        "        self.n_in = 2 * node_features + edge_features\n",
        "        self.n_out = n_out\n",
        "        self.use_global_pooling = use_global_pooling\n",
        "\n",
        "        layers = [\n",
        "            nn.ModuleList([\n",
        "                EdgeInteractionLayer(self.n_in, hidden_channels, latent_channels, aggr=aggr)\n",
        "                for _ in range(n_unshared_layers)\n",
        "            ])\n",
        "        ]\n",
        "        for _ in range(n_layers-1):\n",
        "            layers += [\n",
        "                nn.ModuleList([\n",
        "                    EdgeInteractionLayer(2 * latent_channels * n_unshared_layers + node_features, hidden_channels, latent_channels, aggr=aggr)\n",
        "                    for _ in range(n_unshared_layers)\n",
        "                ])\n",
        "            ]\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "        n_pool = (len(aggr) if isinstance(aggr, list) else 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear((n_unshared_layers * n_pool) * latent_channels, latent_channels, bias=True),\n",
        "            nn.LayerNorm(latent_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_channels, latent_channels, bias=True),\n",
        "            nn.LayerNorm(latent_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_channels, latent_channels, bias=True)\n",
        "        )\n",
        "\n",
        "        self.galaxy_halo_mlp = nn.Sequential(\n",
        "            nn.Linear(latent_channels + node_features, latent_channels, bias=True),\n",
        "            nn.LayerNorm(latent_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_channels, latent_channels, bias=True),\n",
        "            nn.LayerNorm(latent_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_channels, n_out, bias=True)\n",
        "        )\n",
        "\n",
        "        self.D_link = D_link\n",
        "        self.h = 0.\n",
        "\n",
        "    def forward(self, data):\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr\n",
        "\n",
        "        # update hidden state on edge (h, or sometimes e_ij in the text)\n",
        "        h = torch.cat([unshared_layer(data.x, edge_index=edge_index, edge_attr=edge_attr) for unshared_layer in self.layers[0]], axis=1)\n",
        "        self.h = h\n",
        "        h = h.relu()\n",
        "\n",
        "        for layer in self.layers[1:]:\n",
        "            # if more than 1 layer, then also use a skip connection\n",
        "            h = self.h + torch.cat([unshared_layer(h, edge_index=edge_index, edge_attr=edge_attr) for unshared_layer in layer], axis=1)\n",
        "\n",
        "            self.h = h\n",
        "            h = h.relu()\n",
        "\n",
        "        x = torch.concat([self.fc(h), data.x], axis=1) # latent channels + data.x\n",
        "\n",
        "        return (self.galaxy_halo_mlp(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwPhONL2y79D"
      },
      "outputs": [],
      "source": [
        "node_features = data.x.shape[1]\n",
        "edge_features = data.edge_attr.shape[1]\n",
        "\n",
        "\n",
        "bs = 256\n",
        "lr = 3e-2\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "n_layers = 1\n",
        "n_hidden_channels = 32\n",
        "n_latent_channels = 32\n",
        "n_unshared_layers = 8\n",
        "\n",
        "model = EdgeInteractionGNN(\n",
        "    node_features=node_features,\n",
        "    edge_features=edge_features,\n",
        "    n_layers=n_layers,\n",
        "    D_link=D_link,\n",
        "    hidden_channels=n_hidden_channels,\n",
        "    latent_channels=n_latent_channels,\n",
        "    loop=True,\n",
        "    n_unshared_layers=n_unshared_layers,\n",
        "    n_out=1,\n",
        "    aggr=\"sum\"\n",
        ")\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo3CfGW7Hp4Z"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=lr,\n",
        ")\n",
        "\n",
        "train_loader = RandomNodeLoader(\n",
        "    data.subgraph(train_indices),\n",
        "    num_parts=(len(train_indices) // bs),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "valid_loader = RandomNodeLoader(\n",
        "    data.subgraph(valid_indices),\n",
        "    num_parts=(len(valid_indices) // bs),\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPCxIV5n5-IC",
        "outputId": "f280c65d-ef05-484e-9610-23ebb1842c93"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "print(f\"Epoch    Train loss   Valid Loss      RSME \")\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # anneal learning rate\n",
        "    if epoch == int(n_epochs * 0.5):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=lr / 10,\n",
        "        )\n",
        "    elif epoch == int(n_epochs * 0.9):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=lr / 100,\n",
        "        )\n",
        "\n",
        "    train_loss = train(train_loader, model, optimizer, device=device)\n",
        "    valid_loss, p, y  = validate(valid_loader, model, device=device)\n",
        "\n",
        "    print(f\" {epoch + 1: >4d}    {train_loss: >9.5f}    {valid_loss: >9.5f}    {np.sqrt(np.mean((p - y.flatten())**2)): >10.6f}\")\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "lWlUfbqP8Wpr",
        "outputId": "5308b129-21ec-4611-cc6f-a3d6b24251b8"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.sqrt(train_losses), label=\"Train\")\n",
        "plt.plot(np.sqrt(valid_losses), label=\"Valid\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE [dex]\")\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(alpha=0.15)\n",
        "plt.ylim(0.15, 0.25)\n",
        "plt.title(\"Loss curves\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "Wa1T3NNoEYo0",
        "outputId": "4b40f524-0475-48b5-8ace-479b4e380d91"
      },
      "outputs": [],
      "source": [
        "valid_loss, p, y  = validate(valid_loader, model, device=device)\n",
        "\n",
        "plt.figure(figsize=(5, 4), dpi=150)\n",
        "plt.scatter(y, p, s=5, edgecolors=\"none\", c=df.iloc[valid_indices].loghalomass.values, vmin=11, vmax=14)\n",
        "plt.colorbar(label=r\"$\\log(M_{\\rm halo}/M_\\odot)$\")\n",
        "plt.grid(alpha=0.15)\n",
        "plt.xlim(9, 12)\n",
        "plt.ylim(9, 12)\n",
        "plt.xlabel(r\"True $\\log(M_\\bigstar/M_\\odot)$\", fontsize=12)\n",
        "plt.ylabel(r\"GNN Predicted $\\log(M_\\bigstar/M_\\odot)$\", fontsize=12)\n",
        "\n",
        "plt.title(f\"RMSE(Advanced GNN) = {((p.ravel() - y.ravel())**2).mean()**0.5:.4f} dex\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_vvqZ8wRdxT"
      },
      "source": [
        "### **Exercse 6**\n",
        "**Add random noise to features during training. Does it help?**\n",
        "\n",
        "Rationale: Injecting a small amount of random noise to the node and edge features can help the model learn a more robust representation. This method is sometimes called *noise augmentation* or *noise regularization*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXWFaZhD-h_D"
      },
      "source": [
        "<details>\n",
        "<summary><b>Answer</b></summary>\n",
        "\n",
        "Add the following lines to the `train()` train after loading each batch of data:\n",
        "\n",
        "```python\n",
        "if augment:\n",
        "    data_node_features_scatter = 1e-3 * torch.randn_like(data.x) * torch.std(data.x, dim=0)\n",
        "    data_edge_features_scatter = 1e-3 * torch.randn_like(data.edge_attr) * torch.std(data.edge_attr, dim=0)\n",
        "\n",
        "    data.x += data_node_features_scatter\n",
        "    data.edge_attr += data_edge_features_scatter\n",
        "```\n",
        "\n",
        "This also introduces another hyperparameter, that is, the scale of noise added during training time. Here I chose `1e-3`, but you may find that other values are better.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U-CBqsQ_GLv"
      },
      "source": [
        "### **Exercise X**\n",
        "\n",
        "**Design a heterogeneous dataset and graph network that learns different relationships for *central* and *satellite* subhalos.**\n",
        "\n",
        "Sorry, no solution here. This would be an interesting research problem though!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iaa6ZSUGu3Bk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
